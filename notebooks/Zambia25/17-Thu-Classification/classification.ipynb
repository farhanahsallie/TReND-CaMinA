{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f87d91bc",
      "metadata": {
        "id": "f87d91bc"
      },
      "source": [
        "# **Classification**  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/egokcen/TReND-CaMinA/blob/main/notebooks/Zambia25/17-Thu-Classification/classification.ipynb)\n",
        "\n",
        "<img align=\"left\" width=\"100\" src=\"https://raw.githubusercontent.com/trendinafrica/TReND-CaMinA/main/images/CaMinA_logo.png\">\n",
        "\n",
        "### **TReND-CaMinA: Computational Neuroscience and Machine Learning Summer School, Zambia 2025**\n",
        "#### **Content creator:** Evren Gokcen, Carnegie Mellon University\n",
        "\n",
        "---\n",
        "\n",
        "## **Overview**\n",
        "In this lesson, we will learn about **classification** methods. Classification methods are used to predict discrete labels (or classes) from data. Classification is a fundamental task in machine learning and is widely used in neuroscience to decode brain activity patterns.\n",
        "\n",
        "In this notebook, you will decode a monkey's intended arm reach angle from neurons recorded in motor area PMd.\n",
        "\n",
        "<center><img src=\"https://github.com/egokcen/TReND-CaMinA/blob/main/notebooks/Zambia25/17-Thu-Classification/logistic_regression_animation1D.gif?raw=1\" width=500><img src=\"https://github.com/egokcen/TReND-CaMinA/blob/main/notebooks/Zambia25/17-Thu-Classification/logistic_regression_animation2D.gif?raw=1\" width=500></center>\n",
        "\n",
        "## **Learning objectives / key questions**\n",
        "1. Why is it possible to decode external variables (like movement direction, or visual stimulus identity) from neural activity?\n",
        "2. What is binary classification? And what are two fundamental approaches to solving this problem?\n",
        "3. What are decision boundaries and decision regions, and how are they used in classification?\n",
        "4. What is the nearest neighbors method, and how do I implement it for one- and two-dimensional data?\n",
        "5. What is logistic regression, and how do I implement it for one- and two-dimensional data?\n",
        "6. How are nearest neighbors methods and logistic regression similar and different? And what are the strength and limitations of each method?\n",
        "\n",
        "## **Contents**\n",
        "\n",
        "**Core topics:**\n",
        "\n",
        "0. [Import dependencies and data](#dependencies)\n",
        "1. [Introduction: Decoding movement intention in the motor system](#introduction)\n",
        "2. [Nearest neighbors classification](#nearest-neighbors)\n",
        "3. [Logistic regression](#logistic-regression)\n",
        "\n",
        "**Bonus topics:**\n",
        "\n",
        "1. [Population decoding](#population-decoding)\n",
        "2. [Combining classification and PCA](#class-pca)\n",
        "3. [Multiclass classification](#multiclass)\n",
        "4. [Further reading](#further-reading)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbkNfeNCedVK",
      "metadata": {
        "id": "fbkNfeNCedVK"
      },
      "source": [
        "## 0. Import dependencis and data <a name=\"dependencies\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7L6I1xl8ejiy",
      "metadata": {
        "id": "7L6I1xl8ejiy"
      },
      "outputs": [],
      "source": [
        "# If running in Colab, mount the drive folder\n",
        "COLAB = False\n",
        "if COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "92c267a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92c267a8",
        "outputId": "f7f77f08-d640-4f57-d799-1c200450237d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (7.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.7)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.8.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.3.8)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.24.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.26.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy scipy matplotlib ipywidgets\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as spio\n",
        "\n",
        "from collections import Counter\n",
        "from ipywidgets import interact\n",
        "from matplotlib.animation import FuncAnimation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd67331e",
      "metadata": {
        "id": "bd67331e"
      },
      "source": [
        "## Introduction: Decoding movement intention in the motor system <a name=\"introduction\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cc204c1",
      "metadata": {
        "id": "8cc204c1"
      },
      "source": [
        "Let's start where we left off yesterday, with dimensionality reduction.\n",
        "Recall yesterday's data $^1$ (`data_for_exercises.mat`, on the Google Drive), which\n",
        "contains the following variable:\n",
        "- `Xplan`: a 2D array of shape `(n, d)`. Here we have `n = 728` trials and `d = 97`\n",
        "neurons recorded simultaneously in area PMd. `Xplan[i, j]` is the spike count of neuron\n",
        "`j` on trial `i`, taken within a 200 ms bin while the monkey is planning an arm reach.\n",
        "There are 91 trials for each of 8 reaching angles. Trials are ordered by angle, so the\n",
        "first 91 trials are for angle 0, the next 91 trials are for angle 45, and so on.\n",
        "\n",
        "1. The neural data have been generously provided by the laboratory of Prof. Krishna\n",
        "Shenoy at Stanford University. The data are to be used exclusively for educational\n",
        "purposes in this course.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c74f425",
      "metadata": {
        "id": "1c74f425"
      },
      "source": [
        "### Revisiting PCA <a name=\"\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RHhUKatjfPcN",
      "metadata": {
        "id": "RHhUKatjfPcN"
      },
      "source": [
        "#### Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "978c816b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "978c816b",
        "outputId": "7c8d6ca2-7acf-456a-b13c-25ab77a5a32d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Xplan dimensions: (n trials, d neurons)\n",
            "(728, 97)\n"
          ]
        }
      ],
      "source": [
        "# Set paths to data file\n",
        "datapath  = '' # add your path here\n",
        "datafile  = 'data_for_exercises.mat'\n",
        "\n",
        "# Load the Matlab format .mat data into Python\n",
        "mat = spio.loadmat(datapath + datafile, squeeze_me=True)\n",
        "Xplan = mat['Xplan']\n",
        "# Convert the data from uint8 to float64\n",
        "Xplan = Xplan.astype(np.float64)\n",
        "\n",
        "# Check the shape of the data\n",
        "n, d = Xplan.shape\n",
        "print('Xplan dimensions: (n trials, d neurons)')\n",
        "print((n, d))\n",
        "\n",
        "# Other basic constants\n",
        "num_reaches_per_angle = 91\n",
        "num_angles = 8\n",
        "angles = [30.0, 70.0, 110.0, 150.0, 190.0, 230.0, 310.0, 350.0]  # In degrees"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70c84c24",
      "metadata": {
        "id": "70c84c24"
      },
      "source": [
        "#### Compute the sample covariance and diagonalize it (PCA)\n",
        "\n",
        "Recall that the sample covariance matrix\n",
        "$S = \\frac{1}{n} \\sum_i (\\boldsymbol{x}_i-\\boldsymbol{\\mu})(\\boldsymbol{x}_i-\\boldsymbol{\\mu})^{\\intercal}$,\n",
        "and the sample mean $\\boldsymbol{\\mu} = \\frac{1}{n} \\sum_i \\boldsymbol{x}_i$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cdefa44",
      "metadata": {
        "id": "7cdefa44"
      },
      "outputs": [],
      "source": [
        "# First find the mean over trials (axis=0) of each neuron\n",
        "mu = np.mean(Xplan, axis=0)\n",
        "\n",
        "# Now compute the data covariance\n",
        "Xplan_centered = Xplan - mu  # Don't forget to center your data first!\n",
        "cov = (1/n) * Xplan_centered.T @ Xplan_centered  # Now you can compute the covariance\n",
        "\n",
        "# Perform eigendecomposition of the data covariance\n",
        "D, U = np.linalg.eig(cov)  # D = vector of eigenvalues, U = matrix of eigenvectors\n",
        "\n",
        "# Make sure the eigenvalues are sorted (in descending order)\n",
        "idx = np.argsort(D)[::-1]\n",
        "D = D[idx]\n",
        "\n",
        "# Arrange the eigenvectors according to the magnitude of the eigenvalues\n",
        "U = U[:, idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af2070a0",
      "metadata": {
        "id": "af2070a0"
      },
      "source": [
        "#### Project the data onto the top 3 principal components and visualize\n",
        "\n",
        "Recall that projections of the data points $\\boldsymbol{x}_i \\in \\mathbb{R}^d$ into the\n",
        "three-dimensional PC space are given by the PC scores, $\\boldsymbol{z}_i \\in \\mathbb{R}^3$, where $z_k^i = \\boldsymbol{u}_k^{\\intercal} (\\boldsymbol{x}_i - \\boldsymbol{\\mu})$, $k=1,2,3$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6db79a40",
      "metadata": {
        "id": "6db79a40"
      },
      "outputs": [],
      "source": [
        "colors = ['r', 'k', 'y', 'g', 'b', 'm', 'c', 'darkviolet']\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "for reach_idx in range(num_angles):\n",
        "  # Get the trials corresponding to the current reach angle\n",
        "  trial_idxs = np.arange(0,num_reaches_per_angle,1) + (reach_idx)*num_reaches_per_angle\n",
        "  X = Xplan_centered[trial_idxs,:]\n",
        "  ax.scatter(X @ U[:, 0], X @ U[:, 1], X @ U[:, 2], '.', color=colors[reach_idx])\n",
        "\n",
        "ax.set_xlabel('PC 1')\n",
        "ax.set_ylabel('PC 2')\n",
        "ax.set_zlabel('PC 3')\n",
        "\n",
        "plt.title('Reach data projected into three-dimensional PC space');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f685986c",
      "metadata": {
        "id": "f685986c"
      },
      "source": [
        "**_üí¨ Discussion questions:_**\n",
        "\n",
        "1. What is striking about these projections?\n",
        "2. What property of PMd neurons might lead to this pattern?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5afd5d4",
      "metadata": {
        "id": "f5afd5d4"
      },
      "source": [
        "### Tuning curves\n",
        "\n",
        "Let's take a step back from PCA and compute tuning curves for each neuron.\n",
        "Recall that the tuning curve of a neuron is the average firing rate of the neuron (over\n",
        "trials) as a function of the reach angle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aecdfb89",
      "metadata": {
        "id": "aecdfb89"
      },
      "outputs": [],
      "source": [
        "# It's helpful here to reshape the data into a 3D array, with an extra axis for the\n",
        "# reach angle\n",
        "Xplan_angles = Xplan.reshape((num_angles, num_reaches_per_angle, d))\n",
        "\n",
        "# Now we can compute the average firing rate for each neuron at each reach angle\n",
        "tuning_curves = np.mean(Xplan_angles, axis=1)\n",
        "# Compute also the standard error of the mean (SEM) for each neuron at each reach angle\n",
        "sem_tuning_curves = np.std(Xplan_angles, axis=1) / np.sqrt(num_reaches_per_angle)\n",
        "\n",
        "# let's pick a few neurons that might be particularly well tuned\n",
        "num_examples = 3\n",
        "mean_firing_rates = np.mean(tuning_curves, axis=0)\n",
        "# Here we'll choose neurons based on their contribution to one of the PCs\n",
        "top_neurons = np.argsort(np.abs(U[:, 2]))[-num_examples:]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 3))\n",
        "for i in range(num_examples):\n",
        "    # Plot error bands for the SEM\n",
        "    ax.fill_between(\n",
        "        angles,\n",
        "        tuning_curves[:, top_neurons[i]] - sem_tuning_curves[:, top_neurons[i]],\n",
        "        tuning_curves[:, top_neurons[i]] + sem_tuning_curves[:, top_neurons[i]],\n",
        "        alpha=0.2,\n",
        "    )\n",
        "    # Plot the tuning curve\n",
        "    ax.plot(\n",
        "        angles,\n",
        "        tuning_curves[:, top_neurons[i]],\n",
        "        label=f'Neuron {top_neurons[i]}',\n",
        "    )\n",
        "ax.set_xlabel('Reach Angle (deg)')\n",
        "ax.set_ylabel('Average Spike Count')\n",
        "ax.set_title('Example Tuning Curves')\n",
        "ax.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab85e0ec",
      "metadata": {
        "id": "ab85e0ec"
      },
      "source": [
        "**_üí¨ Discussion questions:_**\n",
        "\n",
        "- What is the shape of the tuning curves? How does that relate to the nature of reach angles?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32f75192",
      "metadata": {
        "id": "32f75192"
      },
      "source": [
        "## Nearest neighbors classification <a name=\"nearest-neighbors\"></a>\n",
        "\n",
        "Given how nicely the neural responses appear to be organized according to reach angle,\n",
        "both based on PCA and tuning curves, perhaps we can use this information to\n",
        "decode, or classify, the reach angle from the neural responses.\n",
        "\n",
        "We'll start our journey with the Nearest Neighbors classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c8d3b2e",
      "metadata": {
        "id": "3c8d3b2e"
      },
      "source": [
        "### Binary classification with one neuron\n",
        "\n",
        "To warm up, let's simplify the problem first:\n",
        "1. Let's consider only two reach angles. We are thus interested in\n",
        "   *binary classification*.\n",
        "2. Let's consider only one neuron's response, and see how far we can get in decoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "849cae74",
      "metadata": {
        "id": "849cae74"
      },
      "outputs": [],
      "source": [
        "neuron_idx = 91  # The neuron we'll use for binary classification\n",
        "reach_idx1 = 0  # First reach angle\n",
        "reach_idx2 = 3  # Second reach angle\n",
        "\n",
        "# Extract the data for the two angles\n",
        "X_reach1 = Xplan_angles[reach_idx1, :, neuron_idx]\n",
        "X_reach2 = Xplan_angles[reach_idx2, :, neuron_idx]\n",
        "\n",
        "# Let's create a label vector for the two angles\n",
        "y_reach1 = np.zeros(X_reach1.shape[0])  # Label for the first angle\n",
        "y_reach2 = np.ones(X_reach2.shape[0])  # Label for the second angle\n",
        "\n",
        "# Let's also randomly split the data into training and test sets.\n",
        "# We'll keep the same number of trials for each angle.\n",
        "n_train = int(0.8 * num_reaches_per_angle)  # 80% for training\n",
        "# Seed the random number generator for reproducibility\n",
        "np.random.seed(2)\n",
        "train_idxs = np.random.choice(num_reaches_per_angle, n_train, replace=False)\n",
        "# This function just returns the indices that are not already in the training set\n",
        "test_idxs = np.setdiff1d(np.arange(num_reaches_per_angle), train_idxs)\n",
        "\n",
        "# Train data\n",
        "X_train_reach1 = X_reach1[train_idxs]\n",
        "X_train_reach2 = X_reach2[train_idxs]\n",
        "y_train_reach1 = y_reach1[train_idxs]\n",
        "y_train_reach2 = y_reach2[train_idxs]\n",
        "\n",
        "# Test data\n",
        "X_test_reach1 = X_reach1[test_idxs]\n",
        "X_test_reach2 = X_reach2[test_idxs]\n",
        "y_test_reach1 = y_reach1[test_idxs]\n",
        "y_test_reach2 = y_reach2[test_idxs]\n",
        "\n",
        "# Visualize the responses on each trial\n",
        "plt.figure(figsize=(8, 4))\n",
        "# Spike counts are integer values, and so many data points will overlap.\n",
        "# Let's add an offset and a small amount of jitter to the y-axis, purely for\n",
        "# visualization purposes.\n",
        "plt.plot(\n",
        "    X_train_reach1,\n",
        "    -0.1 + np.random.normal(0, 0.02, size=y_train_reach1.shape),\n",
        "    'r.',\n",
        "    label=f'{angles[reach_idx1]}¬∞ reach',\n",
        ")\n",
        "plt.plot(\n",
        "    X_train_reach2,\n",
        "    0.1 + np.random.normal(0, 0.02, size=y_train_reach2.shape),\n",
        "    'b.',\n",
        "    label=f'{angles[reach_idx2]}¬∞ reach',\n",
        ")\n",
        "plt.ylim(-0.5, 0.5)\n",
        "plt.xlabel('Spike Count')\n",
        "# Remove the y-ticks since they are not informative in this case\n",
        "plt.yticks([])\n",
        "plt.title(f'Responses for Two Reach Angles, Neuron {neuron_idx}')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e75d0f92",
      "metadata": {
        "id": "e75d0f92"
      },
      "source": [
        "It looks like these neural responses have decent separation by the two angles!\n",
        "\n",
        "What if we bring in a few test points?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a730574",
      "metadata": {
        "id": "3a730574"
      },
      "outputs": [],
      "source": [
        "reach1_tests = [0, 8]  # Test trial indices\n",
        "reach2_tests = [0, 1]\n",
        "\n",
        "# Replot the training data\n",
        "plt.figure(figsize=(8, 4))\n",
        "# Spike counts are integer values, and so many data points will overlap.\n",
        "# Let's add an offset and a small amount of jitter to the y-axis, purely for\n",
        "# visualization purposes.\n",
        "plt.plot(\n",
        "    X_train_reach1,\n",
        "    -0.1 + np.random.normal(0, 0.02, size=y_train_reach1.shape),\n",
        "    'r.',\n",
        "    label=f'{angles[reach_idx1]}¬∞ reach',\n",
        ")\n",
        "plt.plot(\n",
        "    X_train_reach2,\n",
        "    0.1 + np.random.normal(0, 0.02, size=y_train_reach2.shape),\n",
        "    'b.',\n",
        "    label=f'{angles[reach_idx2]}¬∞ reach',\n",
        ")\n",
        "# And plot the test points\n",
        "for test_trial_idx in reach1_tests:\n",
        "    plt.plot(X_test_reach1[test_trial_idx], 0.2, '*', markersize=12)\n",
        "for test_trial_idx in reach2_tests:\n",
        "    plt.plot(X_test_reach2[test_trial_idx], 0.2, '*', markersize=12)\n",
        "plt.ylim(-0.5, 0.5)\n",
        "plt.xlabel('Spike Count')\n",
        "# Remove the y-ticks since they are not informative in this case\n",
        "plt.yticks([])\n",
        "plt.title(f'Responses for Two Reach Angles, Neuron {neuron_idx}')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80d5b229",
      "metadata": {
        "id": "80d5b229"
      },
      "source": [
        "**_üí¨ Discussion questions:_**\n",
        "\n",
        "- How would you classify each of these test points? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d86f931b",
      "metadata": {
        "id": "d86f931b"
      },
      "source": [
        "#### Nearest neighbors classification: A simple majority vote\n",
        "\n",
        "We can formalize some of the intuitive principles you likely used above to get the\n",
        "Nearest Neighbors classifier. In essence, the Nearest Neighbors classifier works\n",
        "as follows: given a test data point, find the *k* training data points that are closest\n",
        "to it, and those neighbors \"choose\" the class label of the test point by majority vote.\n",
        "\n",
        "We'll then need three key components to implement this:\n",
        "\n",
        "1. **Distance Metric**: We need a way to measure the distance or similarity between data\n",
        "   points.\n",
        "2. **k-Nearest Neighbors**: For a given test instance, we need to identify the k\n",
        "   training data points that are closest to it.\n",
        "3. **Majority Vote**: We need to identify the most common class label among the k\n",
        "   nearest neighbors and assign that label to the test instance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53372abd",
      "metadata": {
        "id": "53372abd"
      },
      "source": [
        "**_üìù Exercise_**\n",
        "\n",
        "Let's start by creating a base KNN class. Wherever you see a `TODO` (there are **three**), attempt to fill in that line of code.\n",
        "\n",
        "You can find the solution below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lHq2nlXPjN_p",
      "metadata": {
        "id": "lHq2nlXPjN_p"
      },
      "outputs": [],
      "source": [
        "class KNNBase:\n",
        "    def __init__(self, k: int = 3):\n",
        "        \"\"\"\n",
        "        Initialize the KNN classifier with one hyperparameter: k.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        k : int\n",
        "            The number of nearest neighbors to consider for classification. Default: 3.\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
        "        \"\"\"\n",
        "        Fit the model to the training data.\n",
        "\n",
        "        For the KNN classifier, \"fitting\" simply stores the training data and labels.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : numpy array, shape (n, d)\n",
        "            The training data, with n samples and d features.\n",
        "        y : numpy array, shape (n,)\n",
        "            The labels corresponding to the training data.\n",
        "        \"\"\"\n",
        "        self.X_train = []  # TODO\n",
        "        self.y_train = []  # TODO\n",
        "\n",
        "    def distance(self, x1: np.ndarray, x2: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Compute the distance between two points.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x1 : numpy array\n",
        "            The first point.\n",
        "        x2 : numpy array\n",
        "            The second point.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            The distance between the two points.\n",
        "        \"\"\"\n",
        "        # We'll implement this later\n",
        "        pass\n",
        "\n",
        "    def predict(self, x: np.ndarray) -> int:\n",
        "        \"\"\"\n",
        "        Predict the labels for a single input data point.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : numpy array, shape (d,)\n",
        "            A single input data point for which to predict the label.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred : int\n",
        "            The predicted label for the input data point.\n",
        "        \"\"\"\n",
        "        # Compute the distances from the input point to all training points\n",
        "        distances = [self.distance(x, x_train) for x_train in self.X_train]\n",
        "\n",
        "        # Get the closest k neighbors\n",
        "        k_indices = []  # TODO - Hint: Use the np.argsort function\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "\n",
        "        # Majority vote\n",
        "        most_common = Counter(k_nearest_labels).most_common()\n",
        "        return most_common[0][0]\n",
        "\n",
        "    def predict_batch(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predict the labels for a batch of input data points.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : numpy array, shape (n, d)\n",
        "            The input data for which to predict labels.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred : numpy array, shape (n,)\n",
        "            The predicted labels for the input data batch.\n",
        "        \"\"\"\n",
        "        return np.array([self.predict(x) for x in X])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64f43a5b",
      "metadata": {
        "id": "64f43a5b"
      },
      "outputs": [],
      "source": [
        "#@title Double click to see solution {display-mode: \"form\" }\n",
        "class KNNBase:\n",
        "    def __init__(self, k: int = 3):\n",
        "        \"\"\"\n",
        "        Initialize the KNN classifier with one hyperparameter: k.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        k : int\n",
        "            The number of nearest neighbors to consider for classification. Default: 3.\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
        "        \"\"\"\n",
        "        Fit the model to the training data.\n",
        "\n",
        "        For the KNN classifier, \"fitting\" simply stores the training data and labels.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : numpy array, shape (n, d)\n",
        "            The training data, with n samples and d features.\n",
        "        y : numpy array, shape (n,)\n",
        "            The labels corresponding to the training data.\n",
        "        \"\"\"\n",
        "        self.X_train = X  # SOLUTION\n",
        "        self.y_train = y  # SOLUTION\n",
        "\n",
        "    def distance(self, x1: np.ndarray, x2: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Compute the distance between two points.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x1 : numpy array\n",
        "            The first point.\n",
        "        x2 : numpy array\n",
        "            The second point.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            The distance between the two points.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def predict(self, x: np.ndarray) -> int:\n",
        "        \"\"\"\n",
        "        Predict the labels for a single input data point.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : numpy array, shape (d,)\n",
        "            A single input data point for which to predict the label.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred : int\n",
        "            The predicted label for the input data point.\n",
        "        \"\"\"\n",
        "        # Compute the distances from the input point to all training points\n",
        "        distances = [self.distance(x, x_train) for x_train in self.X_train]\n",
        "\n",
        "        # Get the closest k neighbors\n",
        "        k_indices = np.argsort(distances)[:self.k]  # SOLUTION\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "\n",
        "        # Majority vote\n",
        "        most_common = Counter(k_nearest_labels).most_common()\n",
        "        return most_common[0][0]\n",
        "\n",
        "    def predict_batch(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predict the labels for a batch of input data points.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : numpy array, shape (n, d)\n",
        "            The input data for which to predict labels.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred : numpy array, shape (n,)\n",
        "            The predicted labels for the input data batch.\n",
        "        \"\"\"\n",
        "        return np.array([self.predict(x) for x in X])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "105d7177",
      "metadata": {
        "id": "105d7177"
      },
      "source": [
        "#### Distance Metric\n",
        "\n",
        "We now have everything in place except for our distance metric. In our 1D\n",
        "(single-neuron) case, we can simply use the absolute difference between the spike counts\n",
        "of the neuron for the test point and the training points."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0S6AobtBkL18",
      "metadata": {
        "id": "0S6AobtBkL18"
      },
      "source": [
        "**_üìù Exercise_**\n",
        "\n",
        "Implement the distance function below.\n",
        "\n",
        "Wherever you see a `TODO` (there is **one**), attempt to fill in that line of code.\n",
        "\n",
        "You can find the solution below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bJduVMfNkeMq",
      "metadata": {
        "id": "bJduVMfNkeMq"
      },
      "outputs": [],
      "source": [
        "def abs_distance(x1: np.ndarray, x2: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Compute the absolute distance between two 1D data points.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x1 : numpy array\n",
        "        The first data point.\n",
        "    x2 : numpy array\n",
        "        The second data point.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The absolute distance between the two points.\n",
        "    \"\"\"\n",
        "    return None  # TODO: Replace None with your solution (use numpy functions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "062ce3fc",
      "metadata": {
        "id": "062ce3fc"
      },
      "outputs": [],
      "source": [
        "#@title Double click to see solution {display-mode: \"form\" }\n",
        "def abs_distance(x1: np.ndarray, x2: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Compute the absolute distance between two 1D data points.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x1 : numpy array\n",
        "        The first data point.\n",
        "    x2 : numpy array\n",
        "        The second data point.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The absolute distance between the two points.\n",
        "    \"\"\"\n",
        "    return np.abs(x1 - x2)  # SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c95ca083",
      "metadata": {
        "id": "c95ca083"
      },
      "source": [
        "Let's now use this distance metric to implement our first KNN classifier!\n",
        "\n",
        "Instantiate the class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c63fe3",
      "metadata": {
        "id": "97c63fe3"
      },
      "outputs": [],
      "source": [
        "# To instantiate the class, all we need to provide is the number of neighbors k.\n",
        "knn_1D = KNNBase(k=5)\n",
        "# Then we can add the distance metric we just defined.\n",
        "knn_1D.distance = abs_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dHmnsI5lF7K",
      "metadata": {
        "id": "3dHmnsI5lF7K"
      },
      "source": [
        "**_üìù Exercise_**\n",
        "\n",
        "\"Train\" the model!\n",
        "\n",
        "Wherever you see a `TODO` (there is **one**), attempt to fill in that line of code.\n",
        "\n",
        "You can find the solution below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JIGKyNTVlRiY",
      "metadata": {
        "id": "JIGKyNTVlRiY"
      },
      "outputs": [],
      "source": [
        "# Prep the data for compatibility with our KNN class\n",
        "X_train = np.concatenate((X_train_reach1, X_train_reach2))\n",
        "y_train = np.concatenate((y_train_reach1, y_train_reach2))\n",
        "\n",
        "# Fit the KNN model\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f657222",
      "metadata": {
        "id": "4f657222"
      },
      "outputs": [],
      "source": [
        "#@title Double click to see solution {display-mode: \"form\" }\n",
        "# Prep the data for compatibility with our KNN class\n",
        "X_train = np.concatenate((X_train_reach1, X_train_reach2))\n",
        "y_train = np.concatenate((y_train_reach1, y_train_reach2))\n",
        "\n",
        "# Fit the KNN model\n",
        "knn_1D.fit(X_train, y_train)  # SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8eaeeba",
      "metadata": {
        "id": "d8eaeeba"
      },
      "source": [
        "Having trained the model, we can now use it to make predictions on new data.\n",
        "And importantly, we'll need to be able to evaluate how well the model performs on these\n",
        "new unseen data points."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "owWFfNnul1Er",
      "metadata": {
        "id": "owWFfNnul1Er"
      },
      "source": [
        "**_üìù Exercise_**\n",
        "\n",
        "Implement the function to compute classification accuracy.\n",
        "\n",
        "Wherever you see a `TODO` (there is **one**), attempt to fill in that line of code.\n",
        "\n",
        "You can find the solution below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EPxcITfbl-hV",
      "metadata": {
        "id": "EPxcITfbl-hV"
      },
      "outputs": [],
      "source": [
        "# Let's define a function to compute the accuracy of our KNN model\n",
        "def accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Compute the classification accuracy of the model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : numpy array, shape (n,)\n",
        "        The true labels for the data points.\n",
        "    y_pred : numpy array, shape (n,)\n",
        "        The predicted labels for the data points.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The classification accuracy, defined as the proportion of correct predictions.\n",
        "    \"\"\"\n",
        "    return None  # TODO: Replace None with your solution (use numpy functions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aad94fcf",
      "metadata": {
        "id": "aad94fcf"
      },
      "outputs": [],
      "source": [
        "#@title Double click to see solution {display-mode: \"form\" }\n",
        "# Let's define a function to compute the accuracy of our KNN model\n",
        "def accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Compute the classification accuracy of the model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : numpy array, shape (n,)\n",
        "        The true labels for the data points.\n",
        "    y_pred : numpy array, shape (n,)\n",
        "        The predicted labels for the data points.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The classification accuracy, defined as the proportion of correct predictions.\n",
        "    \"\"\"\n",
        "    return np.mean(y_true == y_pred)  # SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "342a8d33",
      "metadata": {
        "id": "342a8d33"
      },
      "source": [
        "Predict labels for the test data and evaluate our model's accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d84cc76d",
      "metadata": {
        "id": "d84cc76d"
      },
      "outputs": [],
      "source": [
        "# Prep the data for compatibility with our KNN class\n",
        "# Train\n",
        "X_train = np.concatenate((X_train_reach1, X_train_reach2))\n",
        "y_train = np.concatenate((y_train_reach1, y_train_reach2))\n",
        "\n",
        "# Test\n",
        "X_test = np.concatenate((X_test_reach1, X_test_reach2))\n",
        "y_test = np.concatenate((y_test_reach1, y_test_reach2))\n",
        "\n",
        "# Predict the labels for the train data\n",
        "y_pred_train = knn_1D.predict_batch(X_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred_test = knn_1D.predict_batch(X_test)\n",
        "\n",
        "# Train accuracy\n",
        "acc_train = accuracy(y_train, y_pred_train)\n",
        "print(f'Train accuracy: {acc_train:.3f}')\n",
        "\n",
        "# Test accuracy\n",
        "acc_test = accuracy(y_test, y_pred_test)\n",
        "print(f'Test accuracy: {acc_test:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e69c26c3",
      "metadata": {
        "id": "e69c26c3"
      },
      "source": [
        "That's quite good! Let's visualize the test points from before, and color them by\n",
        "their predicted class labels. Do these predictions make sense?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "769fd439",
      "metadata": {
        "id": "769fd439"
      },
      "outputs": [],
      "source": [
        "reach1_tests = [0, 8]  # Test trial indices\n",
        "reach2_tests = [0, 1]\n",
        "\n",
        "# Reshape the predicted test labels to include an additional axis for reach angle\n",
        "y_pred_reaches = y_pred_test.reshape(2, -1).T\n",
        "y_pred_reach1 = y_pred_reaches[:, 0]\n",
        "y_pred_reach2 = y_pred_reaches[:, 1]\n",
        "\n",
        "# Replot the training data\n",
        "plt.figure(figsize=(8, 4))\n",
        "# Spike counts are integer values, and so many data points will overlap.\n",
        "# Let's add an offset and a small amount of jitter to the y-axis, purely for\n",
        "# visualization purposes.\n",
        "plt.plot(\n",
        "    X_train_reach1,\n",
        "    -0.1 + np.random.normal(0, 0.02, size=y_train_reach1.shape),\n",
        "    'r.',\n",
        "    label=f'{angles[reach_idx1]}¬∞ reach',\n",
        ")\n",
        "plt.plot(\n",
        "    X_train_reach2,\n",
        "    0.1 + np.random.normal(0, 0.02, size=y_train_reach2.shape),\n",
        "    'b.',\n",
        "    label=f'{angles[reach_idx2]}¬∞ reach',\n",
        ")\n",
        "# And plot the test points with their predicted labels\n",
        "for test_trial_idx in reach1_tests:\n",
        "    color = 'r' if y_pred_reach1[test_trial_idx] == 0 else 'b'\n",
        "    plt.plot(X_test_reach1[test_trial_idx], 0.2, '*', markersize=12, color=color)\n",
        "for test_trial_idx in reach2_tests:\n",
        "    color = 'r' if y_pred_reach2[test_trial_idx] == 0 else 'b'\n",
        "    plt.plot(X_test_reach2[test_trial_idx], 0.2, '*', markersize=12, color=color)\n",
        "plt.ylim(-0.5, 0.5)\n",
        "plt.xlabel('Spike Count')\n",
        "# Remove the y-ticks since they are not informative in this case\n",
        "plt.yticks([])\n",
        "plt.title(f'Responses for Two Reach Angles, Neuron {neuron_idx}')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6899025",
      "metadata": {
        "id": "a6899025"
      },
      "source": [
        "**_üí¨ Discussion questions:_**\n",
        "\n",
        "- Based on the predicted labels shown above, is there a simple rule that\n",
        "   would allow us to classify the test points correctly, without using the KNN\n",
        "   classifier?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2478ec3",
      "metadata": {
        "id": "f2478ec3"
      },
      "source": [
        "#### Decision boundaries, part 1\n",
        "\n",
        "It seems like there might be a critical threshold spike count: Below this threshold,\n",
        "the model predicts one angle, and above it, the other angle.\n",
        "\n",
        "This introduces the concept of a *decision boundary*. Let's visualize our KNN\n",
        "classifier's *decision regions*, and the corresponding decision boundary that results\n",
        "from them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fce9db61",
      "metadata": {
        "id": "fce9db61"
      },
      "outputs": [],
      "source": [
        "# Let's now visualize the decision boundaries of our KNN model\n",
        "# We'll create a grid of points along the x-axis and predict their labels\n",
        "x_min, x_max = X_train.min() - 1, X_train.max() + 1\n",
        "xx = np.linspace(x_min, x_max, 100)\n",
        "y_grid_pred = knn_1D.predict_batch(xx)\n",
        "\n",
        "# Now visualize the predicted labels of the grid points, along with the training data\n",
        "plt.figure(figsize=(8, 4))\n",
        "# Plot the predicted labels of the grid points\n",
        "plt.scatter(xx, np.zeros_like(xx), c=y_grid_pred, cmap='coolwarm_r', alpha=0.3, s=10)\n",
        "# Plot the training data, with a small amount of jitter for visualization\n",
        "plt.plot(\n",
        "    X_train_reach1,\n",
        "    -0.1 + np.random.normal(0, 0.02, size=y_train_reach1.shape),\n",
        "    'r.',\n",
        "    label=f'{angles[reach_idx1]}¬∞ reach',\n",
        ")\n",
        "plt.plot(\n",
        "    X_train_reach2,\n",
        "    0.1 + np.random.normal(0, 0.02, size=y_train_reach2.shape),\n",
        "    'b.',\n",
        "    label=f'{angles[reach_idx2]}¬∞ reach',\n",
        ")\n",
        "plt.ylim(-0.5, 0.5)\n",
        "plt.xlabel('Spike Count')\n",
        "# Remove the y-ticks since they are not informative in this case\n",
        "plt.yticks([])\n",
        "plt.title('Decision Boundaries of the 1D KNN Model')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6be20088",
      "metadata": {
        "id": "6be20088"
      },
      "source": [
        "**_üí¨ Discussion questions:_**\n",
        "\n",
        "- Does this decision boundary align with our test predictions, above?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd0a29b3",
      "metadata": {
        "id": "cd0a29b3"
      },
      "source": [
        "### Binary classification with two neurons\n",
        "\n",
        "We're now ready to extend our Nearest Neighbors classifier to two neurons!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "293f3564",
      "metadata": {
        "id": "293f3564"
      },
      "outputs": [],
      "source": [
        "# Let's start by visualizing the training data for two neurons\n",
        "neuron_idxs = [91, 34]  # The neurons we'll use for binary classification\n",
        "reach_idx1 = 0  # First reach angle\n",
        "reach_idx2 = 3  # Second reach angle\n",
        "\n",
        "# Extract the data for the two angles\n",
        "X_reach1 = Xplan_angles[reach_idx1, :, neuron_idxs].T\n",
        "X_reach2 = Xplan_angles[reach_idx2, :, neuron_idxs].T\n",
        "\n",
        "# Let's create a label vector for the two angles\n",
        "y_reach1 = np.zeros(X_reach1.shape[0])  # Label for the first angle\n",
        "y_reach2 = np.ones(X_reach2.shape[0])  # Label for the second angle\n",
        "\n",
        "# Let's also randomly split the data into training and test sets.\n",
        "# We'll keep the same number of trials for each angle.\n",
        "n_train = int(0.8 * num_reaches_per_angle)  # 80% for training\n",
        "# Seed the random number generator for reproducibility\n",
        "np.random.seed(1)\n",
        "train_idxs = np.random.choice(num_reaches_per_angle, n_train, replace=False)\n",
        "# This function just returns the indices that are not already in the training set\n",
        "test_idxs = np.setdiff1d(np.arange(num_reaches_per_angle), train_idxs)\n",
        "\n",
        "# Train data\n",
        "X_train_reach1 = X_reach1[train_idxs]\n",
        "X_train_reach2 = X_reach2[train_idxs]\n",
        "y_train_reach1 = y_reach1[train_idxs]\n",
        "y_train_reach2 = y_reach2[train_idxs]\n",
        "\n",
        "# Test data\n",
        "X_test_reach1 = X_reach1[test_idxs]\n",
        "X_test_reach2 = X_reach2[test_idxs]\n",
        "y_test_reach1 = y_reach1[test_idxs]\n",
        "y_test_reach2 = y_reach2[test_idxs]\n",
        "\n",
        "# Visualize the responses on each trial\n",
        "plt.figure(figsize=(6, 5))\n",
        "# Spike counts are integer values, and so many data points will overlap.\n",
        "# Let's add an offset and a small amount of jitter to the y-axis, purely for\n",
        "# visualization purposes.\n",
        "plt.plot(\n",
        "    X_train_reach1[:, 0] + np.random.normal(0, 0.05, size=X_train_reach1.shape[0]),\n",
        "    X_train_reach1[:, 1] + np.random.normal(0, 0.05, size=X_train_reach1.shape[0]),\n",
        "    'r.',\n",
        "    label=f'{angles[reach_idx1]}¬∞ reach',\n",
        ")\n",
        "plt.plot(\n",
        "    X_train_reach2[:, 0] + np.random.normal(0, 0.05, size=X_train_reach2.shape[0]),\n",
        "    X_train_reach2[:, 1] + np.random.normal(0, 0.05, size=X_train_reach2.shape[0]),\n",
        "    'b.',\n",
        "    label=f'{angles[reach_idx2]}¬∞ reach',\n",
        ")\n",
        "plt.xlabel(f'Spike Count, Neuron {neuron_idxs[0]}')\n",
        "plt.ylabel(f'Spike Count, Neuron {neuron_idxs[1]}')\n",
        "plt.title(f'Responses for Two Reach Angles, Neurons {neuron_idxs}')\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39637a78",
      "metadata": {
        "id": "39637a78"
      },
      "source": [
        "This looks highly separable! How should we modify our KNN class to handle two neurons?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WnxnRtLzojtA",
      "metadata": {
        "id": "WnxnRtLzojtA"
      },
      "source": [
        "**_üìù Exercise_**\n",
        "\n",
        "Implement the function to compute Euclidean distance between two points.\n",
        "\n",
        "Wherever you see a `TODO` (there is **one**), attempt to fill in that line of code.\n",
        "\n",
        "You can find the solution below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8QUmPEUeo4Qq",
      "metadata": {
        "id": "8QUmPEUeo4Qq"
      },
      "outputs": [],
      "source": [
        "def euclidean_distance(x1: np.ndarray, x2: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Compute the Euclidean distance between two data points.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x1 : numpy array\n",
        "        The first data point.\n",
        "    x2 : numpy array\n",
        "        The second data point.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The Euclidean distance between the two points.\n",
        "    \"\"\"\n",
        "    return None  # TODO: Replace None with your solution (use numpy functions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4967c6e",
      "metadata": {
        "id": "a4967c6e"
      },
      "outputs": [],
      "source": [
        "#@title Double click to see solution {display-mode: \"form\" }\n",
        "def euclidean_distance(x1: np.ndarray, x2: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Compute the Euclidean distance between two data points.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x1 : numpy array\n",
        "        The first data point.\n",
        "    x2 : numpy array\n",
        "        The second data point.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The Euclidean distance between the two points.\n",
        "    \"\"\"\n",
        "    return np.sqrt(np.sum((x1 - x2) ** 2))  # SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aa6c987",
      "metadata": {
        "id": "9aa6c987"
      },
      "source": [
        "We can now take the same steps as before to create and train our KNN classifier.\n",
        "\n",
        "Instantiate the class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a720d54e",
      "metadata": {
        "id": "a720d54e"
      },
      "outputs": [],
      "source": [
        "knn_2D = KNNBase(k=5)\n",
        "knn_2D.distance = euclidean_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f865560",
      "metadata": {
        "id": "6f865560"
      },
      "source": [
        "\"Train\" the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a861b7e0",
      "metadata": {
        "id": "a861b7e0"
      },
      "outputs": [],
      "source": [
        "# Prep the data for compatibility with our KNN class\n",
        "X_train = np.concatenate((X_train_reach1, X_train_reach2))\n",
        "y_train = np.concatenate((y_train_reach1, y_train_reach2))\n",
        "\n",
        "# Fit the KNN model\n",
        "knn_2D.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03aa1e5f",
      "metadata": {
        "id": "03aa1e5f"
      },
      "source": [
        "Predict labels for the test data and evaluate our model's accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a50e13e4",
      "metadata": {
        "id": "a50e13e4"
      },
      "outputs": [],
      "source": [
        "# Prep the data for compatibility with our KNN class\n",
        "# Train\n",
        "X_train = np.concatenate((X_train_reach1, X_train_reach2))\n",
        "y_train = np.concatenate((y_train_reach1, y_train_reach2))\n",
        "\n",
        "# Test\n",
        "X_test = np.concatenate((X_test_reach1, X_test_reach2))\n",
        "y_test = np.concatenate((y_test_reach1, y_test_reach2))\n",
        "\n",
        "# Predict the labels for the train data\n",
        "y_pred_train = knn_2D.predict_batch(X_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred_test = knn_2D.predict_batch(X_test)\n",
        "\n",
        "# Train accuracy\n",
        "acc_train = accuracy(y_train, y_pred_train)\n",
        "print(f'Train accuracy: {acc_train:.3f}')\n",
        "\n",
        "# Test accuracy\n",
        "acc_test = accuracy(y_test, y_pred_test)\n",
        "print(f'Test accuracy: {acc_test:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba665f25",
      "metadata": {
        "id": "ba665f25"
      },
      "source": [
        "That's again quite good!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce911bdb",
      "metadata": {
        "id": "ce911bdb"
      },
      "source": [
        "#### Decision boundaries, part 2\n",
        "\n",
        "Let's revisit the idea of decision boundaries, but now for our two-dimensional data.\n",
        "\n",
        "**_üí¨ Discussion questions:_**\n",
        "- How would you draw the decision boundary for our two-neuron KNN\n",
        "classifier?\n",
        "\n",
        "Then let's go ahead and visualize the true decision boundary for our classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a81efe70",
      "metadata": {
        "id": "a81efe70"
      },
      "outputs": [],
      "source": [
        "# Let's now visualize the decision boundaries of our KNN model\n",
        "# We'll create a 2D grid of points and predict their labels\n",
        "x_min, x_max = np.min(X_train[:, 0]) - 1, np.max(X_train[:, 0]) + 1\n",
        "y_min, y_max = np.min(X_train[:, 1]) - 1, np.max(X_train[:, 1]) + 1\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 50), np.linspace(y_min, y_max, 50))\n",
        "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "y_grid_pred = knn_2D.predict_batch(grid_points)\n",
        "\n",
        "# Now visualize the predicted labels of the grid points, along with the training data\n",
        "plt.figure(figsize=(6, 5))\n",
        "# Plot the decision boundary\n",
        "plt.scatter(\n",
        "    grid_points[:, 0],\n",
        "    grid_points[:, 1],\n",
        "    c=y_grid_pred,\n",
        "    cmap='coolwarm_r',\n",
        "    alpha=0.3,\n",
        "    s=5,\n",
        ")\n",
        "# Plot the training data, with a small amount of jitter for visualization\n",
        "plt.plot(\n",
        "    X_train_reach1[:, 0] + np.random.normal(0, 0.05, size=X_train_reach1.shape[0]),\n",
        "    X_train_reach1[:, 1] + np.random.normal(0, 0.05, size=X_train_reach1.shape[0]),\n",
        "    'r.',\n",
        "    label=f'{angles[reach_idx1]}¬∞ reach',\n",
        ")\n",
        "plt.plot(\n",
        "    X_train_reach2[:, 0] + np.random.normal(0, 0.05, size=X_train_reach2.shape[0]),\n",
        "    X_train_reach2[:, 1] + np.random.normal(0, 0.05, size=X_train_reach2.shape[0]),\n",
        "    'b.',\n",
        "    label=f'{angles[reach_idx2]}¬∞ reach',\n",
        ")\n",
        "plt.xlabel(f'Spike Count, Neuron {neuron_idxs[0]}')\n",
        "plt.ylabel(f'Spike Count, Neuron {neuron_idxs[1]}')\n",
        "plt.title('Decision Boundaries of the 2D KNN Model')\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d66b6700",
      "metadata": {
        "id": "d66b6700"
      },
      "source": [
        "**_üìù Exercises_**\n",
        "\n",
        "1. Rerun the above analyses, but try different values for $k$. What happens to the decision boundary as $k$ changes? How does $k$ control the effective complexity of the classifier?\n",
        "2. Rerun the above analyses, but try different pairs of reach angles to classify. Which pairs result in the best classification performance, and why? Which pairs result in the the worst classification performance, and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22da5545",
      "metadata": {
        "id": "22da5545"
      },
      "source": [
        "## Logistic Regression: Parametrizing the decision boundary directly <a name=\"logistic-regression\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d7af6b7",
      "metadata": {
        "id": "0d7af6b7"
      },
      "source": [
        "You may have noticed that, for certain values of $k$, the decision boundary looks\n",
        "essentially like a straight line. Perhaps we can do well, then, by directly choosing\n",
        "the decision boundary to be linear, and then estimating the best slope and intercept\n",
        "of that line from the data. This brings us to logistic regression."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c040f82",
      "metadata": {
        "id": "9c040f82"
      },
      "source": [
        "### One-dimensional decision boundaries: A simple threshold value\n",
        "\n",
        "Let's return to the one-dimensional, single-neuron case. Again, we want to classify two reach angles, using the spike counts from one neuron.\n",
        "\n",
        "This time, let's set a threshold value, $b$ directly, with the following rule:\n",
        "$$\\text{If } x < b, \\text{ then predict angle 1. Else, predict angle 2.}$$\n",
        "\n",
        "Note: This threshold value $b$ is frequently referred to as the *bias* term in machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "493ef1a8",
      "metadata": {
        "id": "493ef1a8"
      },
      "outputs": [],
      "source": [
        "#@title Let's prepare the 1D data as before {display-mode: \"form\" }\n",
        "neuron_idx = 91  # The neuron we'll use for binary classification\n",
        "reach_idx1 = 0  # First reach angle\n",
        "reach_idx2 = 3  # Second reach angle\n",
        "\n",
        "# Extract the data for the two angles\n",
        "X_reach1 = Xplan_angles[reach_idx1, :, neuron_idx]\n",
        "X_reach2 = Xplan_angles[reach_idx2, :, neuron_idx]\n",
        "\n",
        "# Let's create a label vector for the two angles\n",
        "y_reach1 = np.zeros(X_reach1.shape[0])  # Label for the first angle\n",
        "y_reach2 = np.ones(X_reach2.shape[0])  # Label for the second angle\n",
        "\n",
        "# Let's also randomly split the data into training and test sets.\n",
        "# We'll keep the same number of trials for each angle.\n",
        "n_train = int(0.8 * num_reaches_per_angle)  # 80% for training\n",
        "# Seed the random number generator for reproducibility\n",
        "np.random.seed(2)\n",
        "train_idxs = np.random.choice(num_reaches_per_angle, n_train, replace=False)\n",
        "# This function just returns the indices that are not already in the training set\n",
        "test_idxs = np.setdiff1d(np.arange(num_reaches_per_angle), train_idxs)\n",
        "\n",
        "# Train data\n",
        "X_train_reach1 = X_reach1[train_idxs]\n",
        "X_train_reach2 = X_reach2[train_idxs]\n",
        "y_train_reach1 = y_reach1[train_idxs]\n",
        "y_train_reach2 = y_reach2[train_idxs]\n",
        "X_train = np.concatenate((X_train_reach1, X_train_reach2))\n",
        "y_train = np.concatenate((y_train_reach1, y_train_reach2))\n",
        "\n",
        "# Test data\n",
        "X_test_reach1 = X_reach1[test_idxs]\n",
        "X_test_reach2 = X_reach2[test_idxs]\n",
        "y_test_reach1 = y_reach1[test_idxs]\n",
        "y_test_reach2 = y_reach2[test_idxs]\n",
        "X_test = np.concatenate((X_test_reach1, X_test_reach2))\n",
        "y_test = np.concatenate((y_test_reach1, y_test_reach2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0e92e60",
      "metadata": {
        "id": "d0e92e60"
      },
      "source": [
        "Then let's play around with a threshold ourselves, and see how it affects the decision boundary and the classification performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8849880e",
      "metadata": {
        "id": "8849880e"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import interact\n",
        "import ipywidgets as widgets\n",
        "\n",
        "@interact(b=widgets.FloatSlider(min=0, max=12, step=0.5, value=2))\n",
        "def plot_decision_boundary(b):\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    # Replot the training data\n",
        "    # Spike counts are integer values, and so many data points will overlap.\n",
        "    # Let's add an offset and a small amount of jitter to the y-axis, purely for\n",
        "    # visualization purposes.\n",
        "    # Seed the random number generator for reproducibility\n",
        "    np.random.seed(0)\n",
        "    plt.plot(\n",
        "        X_train_reach1,\n",
        "        -0.1 + np.random.normal(0, 0.02, size=y_train_reach1.shape),\n",
        "        'r.',\n",
        "        label=f'{angles[reach_idx1]}¬∞ reach',\n",
        "    )\n",
        "    plt.plot(\n",
        "        X_train_reach2,\n",
        "        0.1 + np.random.normal(0, 0.02, size=y_train_reach2.shape),\n",
        "        'b.',\n",
        "        label=f'{angles[reach_idx2]}¬∞ reach',\n",
        "    )\n",
        "    # Plot the decision boundary\n",
        "    plt.axvline(x=b, color='m', linestyle='--', label=f'Threshold b={b:.1f}')\n",
        "\n",
        "    # Let's also compute classification accuracy for the current threshold\n",
        "\n",
        "    # Predict the labels for the train data\n",
        "    y_pred_train = (X_train > b).astype(int)\n",
        "\n",
        "    # Predict the labels for the test data\n",
        "    y_pred_test = (X_test > b).astype(int)\n",
        "\n",
        "    # Train accuracy\n",
        "    acc_train = accuracy(y_train, y_pred_train)\n",
        "\n",
        "    # Test accuracy\n",
        "    acc_test = accuracy(y_test, y_pred_test)\n",
        "\n",
        "    # Final plot formatting\n",
        "    plt.ylim(-0.5, 0.5)\n",
        "    plt.xlabel('Spike Count')\n",
        "    # Remove the y-ticks since they are not informative in this case\n",
        "    plt.yticks([])\n",
        "    plt.title(f\"Decision Threshold b={b:.1f}; Train acc: {acc_train:.3f}, Test acc: {acc_test:.3f}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27443361",
      "metadata": {
        "id": "27443361"
      },
      "source": [
        "**_üí¨ Discussion questions:_**\n",
        "\n",
        "1. At which values of $b$ does this classifier perform as well as the KNN classifier (with $k = 5$)?\n",
        "2. Are there values of $b$ where this classifier performs even better than the KNN classifier?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c139cfb",
      "metadata": {
        "id": "8c139cfb"
      },
      "source": [
        "#### **Note:** Parametric vs non-parametric classifiers\n",
        "\n",
        "This simple threshold classifier is our first example of a *parametric classifier*.\n",
        "The parameter, in this case, is the threshold value $b$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbab268b",
      "metadata": {
        "id": "cbab268b"
      },
      "source": [
        "### Two-dimensional decision boundaries: A line with a slope and intercept\n",
        "\n",
        "Let's now extend this idea to two dimensions. We can represent the decision boundary as a line with a slope and intercept:\n",
        "$$y = wx + b$$\n",
        "\n",
        "where $w$ is the slope and $b$ is the intercep, or bias. And from this line, we can define\n",
        "the decision rule as follows:\n",
        "$$\\text{If } y < wx + b, \\text{ then predict angle 1. Else, predict angle 2.}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6fa7d5f",
      "metadata": {
        "id": "e6fa7d5f"
      },
      "outputs": [],
      "source": [
        "#@title Let's prepare the 2D data as before {display-mode: \"form\" }\n",
        "\n",
        "neuron_idxs = [91, 34]  # The neurons we'll use for binary classification\n",
        "reach_idx1 = 0  # First reach angle\n",
        "reach_idx2 = 3  # Second reach angle\n",
        "\n",
        "# Extract the data for the two angles\n",
        "X_reach1 = Xplan_angles[reach_idx1, :, neuron_idxs].T\n",
        "X_reach2 = Xplan_angles[reach_idx2, :, neuron_idxs].T\n",
        "\n",
        "# Let's create a label vector for the two angles\n",
        "y_reach1 = np.zeros(X_reach1.shape[0])  # Label for the first angle\n",
        "y_reach2 = np.ones(X_reach2.shape[0])  # Label for the second angle\n",
        "\n",
        "# Let's also randomly split the data into training and test sets.\n",
        "# We'll keep the same number of trials for each angle.\n",
        "n_train = int(0.8 * num_reaches_per_angle)  # 80% for training\n",
        "# Seed the random number generator for reproducibility\n",
        "np.random.seed(2)\n",
        "train_idxs = np.random.choice(num_reaches_per_angle, n_train, replace=False)\n",
        "# This function just returns the indices that are not already in the training set\n",
        "test_idxs = np.setdiff1d(np.arange(num_reaches_per_angle), train_idxs)\n",
        "\n",
        "# Train data\n",
        "X_train_reach1 = X_reach1[train_idxs]\n",
        "X_train_reach2 = X_reach2[train_idxs]\n",
        "y_train_reach1 = y_reach1[train_idxs]\n",
        "y_train_reach2 = y_reach2[train_idxs]\n",
        "X_train = np.concatenate((X_train_reach1, X_train_reach2))\n",
        "y_train = np.concatenate((y_train_reach1, y_train_reach2))\n",
        "\n",
        "# Test data\n",
        "X_test_reach1 = X_reach1[test_idxs]\n",
        "X_test_reach2 = X_reach2[test_idxs]\n",
        "y_test_reach1 = y_reach1[test_idxs]\n",
        "y_test_reach2 = y_reach2[test_idxs]\n",
        "X_test = np.concatenate((X_test_reach1, X_test_reach2))\n",
        "y_test = np.concatenate((y_test_reach1, y_test_reach2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "315d4bad",
      "metadata": {
        "id": "315d4bad"
      },
      "source": [
        "Then we can now play around with the slope $w$ and the intercept $b$, and see how they affect the decision boundary and the classification performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "610c632c",
      "metadata": {
        "id": "610c632c"
      },
      "outputs": [],
      "source": [
        "# Let's play around with both the slope, w, and the intercept, b, of the decision boundary\n",
        "@interact(w=widgets.FloatSlider(min=-2, max=2, step=0.1, value=-0.2),\n",
        "          b=widgets.FloatSlider(min=0, max=15, step=0.5, value=5))\n",
        "def plot_decision_boundary(w, b):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    # Replot the training data\n",
        "    # Spike counts are integer values, and so many data points will overlap.\n",
        "    # Let's add an offset and a small amount of jitter to the y-axis, purely for\n",
        "    # visualization purposes.\n",
        "    # Seed the random number generator for reproducibility\n",
        "    np.random.seed(0)\n",
        "    plt.plot(\n",
        "        X_train_reach1[:, 0] + np.random.normal(0, 0.05, size=X_train_reach1.shape[0]),\n",
        "        X_train_reach1[:, 1] + np.random.normal(0, 0.05, size=X_train_reach1.shape[0]),\n",
        "        'r.',\n",
        "        label=f'{angles[reach_idx1]}¬∞ reach',\n",
        "    )\n",
        "    plt.plot(\n",
        "        X_train_reach2[:, 0] + np.random.normal(0, 0.05, size=X_train_reach2.shape[0]),\n",
        "        X_train_reach2[:, 1] + np.random.normal(0, 0.05, size=X_train_reach2.shape[0]),\n",
        "        'b.',\n",
        "        label=f'{angles[reach_idx2]}¬∞ reach',\n",
        "    )\n",
        "    # Plot the decision boundary\n",
        "    x_vals = np.linspace(X_train[:, 0].min(), X_train[:, 0].max()-3, 100)\n",
        "    y_vals = w * x_vals + b\n",
        "    plt.plot(x_vals, y_vals, 'm--', label='Decision Boundary')\n",
        "\n",
        "    # Let's also compute classification accuracy for the current slope and intercept\n",
        "\n",
        "    # Predict the labels for the train data\n",
        "    y_pred_train = (X_train @ np.array([w, -1]) + b < 0).astype(int)\n",
        "\n",
        "    # Predict the labels for the test data\n",
        "    y_pred_test = (X_test @ np.array([w, -1]) + b < 0).astype(int)\n",
        "\n",
        "    # Train accuracy\n",
        "    acc_train = accuracy(y_train, y_pred_train)\n",
        "\n",
        "    # Test accuracy\n",
        "    acc_test = accuracy(y_test, y_pred_test)\n",
        "\n",
        "    # Final plot formatting\n",
        "    plt.xlabel(f'Spike Count, Neuron {neuron_idxs[0]}')\n",
        "    plt.ylabel(f'Spike Count, Neuron {neuron_idxs[1]}')\n",
        "    plt.title(f\"Decision Boundary: y={w:.2f}x + {b:.1f}; Train acc: {acc_train:.3f}, Test acc: {acc_test:.3f}\")\n",
        "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a701cc68",
      "metadata": {
        "id": "a701cc68"
      },
      "source": [
        "**_üí¨ Discussion questions:_**\n",
        "\n",
        "1. How many parameters does our classifier now have?\n",
        "2. At which values of the slope, $w$, and the intercept, $b$, does this classifier perform as well as the KNN classifier (with $k = 5$)?\n",
        "3. Are there values of $w$ and $b$ where this classifier performs even better than the KNN classifier?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec2d575d",
      "metadata": {
        "id": "ec2d575d"
      },
      "source": [
        "### Toward logistic regression\n",
        "\n",
        "Our approach so far has been to manually set the slope and intercept of the decision boundary, and then to classify data points in a binary manner based on whether they fall above or below this line.\n",
        "\n",
        "**_üí¨ Discussion questions:_**\n",
        "\n",
        "What are the drawbacks of this approach? Consider the following prompts:\n",
        "\n",
        "1. Should points very close to the decision boundary be treated the same as points far away from it?\n",
        "2. What if the data are more complex? Especially, if the data are not clearly separable, and if we start considering more than two neurons?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adafe48b",
      "metadata": {
        "id": "adafe48b"
      },
      "source": [
        "#### Probabilities and the logistic sigmoid function\n",
        "\n",
        "To address the first limitation, we'll start working with probabilities.\n",
        "Specifically, we want to be able to answer the following question:\n",
        "\n",
        "*Given a data point, what is the probability that it belongs to class (reach angle) 1?*\n",
        "\n",
        "In math terms, we would write this statement as:\n",
        "$$P(C_1 | \\mathbf{x})$$\n",
        "\n",
        "Review question: What key properties define a probability?\n",
        "- A probability is a value between 0 and 1.\n",
        "- The sum of probabilities for all possible classes must equal 1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1fa71e7",
      "metadata": {
        "id": "d1fa71e7"
      },
      "source": [
        "#### The logistic sigmoid function\n",
        "\n",
        "This brings us to the logistic sigmoid function, which is defined as:\n",
        "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gA56jnauwfEi",
      "metadata": {
        "id": "gA56jnauwfEi"
      },
      "source": [
        "**_üìù Exercise_**\n",
        "\n",
        "To gain intuition, let's implement the logistic sigmoid in code.\n",
        "\n",
        "Wherever you see a `TODO` (there is **one**), attempt to fill in that line of code.\n",
        "\n",
        "You can find the solution below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EhgEyG7UwoIH",
      "metadata": {
        "id": "EhgEyG7UwoIH"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z: float) -> float:\n",
        "    \"\"\"\n",
        "    Compute the logistic sigmoid function for a given input z.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    z : float\n",
        "        The input value for which to compute the sigmoid.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The output of the sigmoid function.\n",
        "    \"\"\"\n",
        "    return None  # TODO: Replace None with your solution (use numpy functions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34873b2a",
      "metadata": {
        "id": "34873b2a"
      },
      "outputs": [],
      "source": [
        "#@title Double click to see solution {display-mode: \"form\" }\n",
        "def sigmoid(z: float) -> float:\n",
        "    \"\"\"\n",
        "    Compute the logistic sigmoid function for a given input z.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    z : float\n",
        "        The input value for which to compute the sigmoid.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The output of the sigmoid function.\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-z))  # SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a93d75be",
      "metadata": {
        "id": "a93d75be"
      },
      "outputs": [],
      "source": [
        "# Let's plot the sigmoid function for a range of z values\n",
        "z_values = np.linspace(-10, 10, 100)\n",
        "sigmoid_values = sigmoid(z_values)\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.axhline(0.5, color='gray', linestyle='--')\n",
        "plt.axvline(0, color='gray', linestyle='--')\n",
        "plt.plot(z_values, sigmoid_values, color='b')\n",
        "plt.title('Logistic Sigmoid Function')\n",
        "plt.xlabel('z')\n",
        "plt.ylabel('sigmoid(z)')\n",
        "plt.ylim(-0.1, 1.1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b92fd53a",
      "metadata": {
        "id": "b92fd53a"
      },
      "source": [
        "This function is promising so far: for all input values $z$, it outputs a value between 0 and 1, which is exactly what we want for a probability.\n",
        "\n",
        "But now, let's add another definition on top of $z$:\n",
        "$$z = w \\cdot x + b$$\n",
        "\n",
        "So $z$ is now a linear function of an input variable $x$, with a weight parameter $w$ and a bias parameter $b$.\n",
        "\n",
        "Let's now replot the logistic sigmoid function, but this time as a function of $x$,\n",
        "rather than $z$. Specifically,\n",
        "$$\\sigma(w \\cdot x + b) = \\frac{1}{1 + e^{-(w \\cdot x + b)}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57fdf231",
      "metadata": {
        "id": "57fdf231"
      },
      "outputs": [],
      "source": [
        "# Let's play around with the logistic sigmoid as a function of x, as the parameters\n",
        "# w and b change\n",
        "@interact(w=widgets.FloatSlider(min=-5, max=5, step=0.05, value=1.0),\n",
        "          b=widgets.FloatSlider(min=-5, max=5, step=0.5, value=0))\n",
        "def plot_sigmoid(w, b):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    # Compute the sigmoid function for a range of x values\n",
        "    x_vals = np.linspace(-10, 10, 100)\n",
        "    z_vals = w * x_vals + b\n",
        "    sigmoid_vals = sigmoid(z_vals)\n",
        "\n",
        "    # Plot the sigmoid function\n",
        "    plt.axhline(0.5, color='gray', linestyle='--')\n",
        "    # Plot the vertical line where the sigmoid crosses 0.5\n",
        "    plt.axvline(-b/w if w != 0 else 0, color='gray', linestyle='--')\n",
        "    plt.plot(x_vals, sigmoid_vals, color='b')\n",
        "\n",
        "    plt.title(f'Sigmoid Function: w={w:.2f}, b={b:.1f}')\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('sigmoid(wx + b)')\n",
        "    plt.ylim(-0.1, 1.1)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94f89f89",
      "metadata": {
        "id": "94f89f89"
      },
      "source": [
        "**_üí¨ Discussion questions:_**\n",
        "\n",
        "1. What effect does the weight $w$ have on the shape of the logistic sigmoid function? What happens when the magnitude of $w$ is very large or very small?\n",
        "2. What effect does the bias $b$ have on the shape of the logistic sigmoid function?\n",
        "3. What happens when $w$ becomes negative?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c49cde3d",
      "metadata": {
        "id": "c49cde3d"
      },
      "source": [
        "#### Class probabilities\n",
        "\n",
        "Here's the key point: as a function of our data points $x$, the logistic sigmoid\n",
        "satisfies the key properties for defining probabilities. We will then define the probability of class 1 as follows:\n",
        "$$P(C_1 | \\mathbf{x}) = \\sigma(w \\cdot x + b) = \\frac{1}{1 + e^{-(w \\cdot x + b)}}$$\n",
        "\n",
        "**_üí¨ Discussion questions:_**\n",
        "\n",
        "How do we define the probability of class 2?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abaefc35",
      "metadata": {
        "id": "abaefc35"
      },
      "outputs": [],
      "source": [
        "# As an extra demo, let's visualize class probabilities for two classes, based on the\n",
        "# logistic sigmoid function\n",
        "@interact(w=widgets.FloatSlider(min=-5, max=5, step=0.05, value=1.0),\n",
        "          b=widgets.FloatSlider(min=-5, max=5, step=0.5, value=0))\n",
        "def plot_class_probabilities(w, b):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    # Compute the sigmoid function for a range of x values\n",
        "    x_vals = np.linspace(-10, 10, 100)\n",
        "    z_vals = w * x_vals + b\n",
        "    sigmoid_vals = sigmoid(z_vals)\n",
        "\n",
        "    # Plot the sigmoid function\n",
        "    plt.axhline(0.5, color='gray', linestyle='--')\n",
        "    # Plot the vertical line where the sigmoid crosses 0.5\n",
        "    plt.axvline(-b/w if w != 0 else 0, color='gray', linestyle='--')\n",
        "    # Probability of Class 1\n",
        "    plt.plot(x_vals, sigmoid_vals, color='b', label='Probability of Class 1')\n",
        "    # Probability of Class 2\n",
        "    plt.plot(x_vals, 1 - sigmoid_vals, color='r', label='Probability of Class 2')\n",
        "\n",
        "    plt.title(f'Class Probabilities: w={w:.2f}, b={b:.1f}')\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('Class probability')\n",
        "    plt.ylim(-0.1, 1.1)\n",
        "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9df4ba18",
      "metadata": {
        "id": "9df4ba18"
      },
      "source": [
        "#### A logistic classifier\n",
        "\n",
        "With the logistic sigmoid function, we now have a way to define the probability that a data point belongs to each class.\n",
        "\n",
        "How do we use these probabilities to classify the data points? We can set a threshold on the probability:\n",
        "$$\\text{If } P(C_1 | \\mathbf{x}) > 0.5, \\text{ then predict class 1. Else, predict class 2.}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a74c132",
      "metadata": {
        "id": "1a74c132"
      },
      "source": [
        "And now let's start applying this logistic sigmoid classifier to our data\n",
        "points, starting with the 1D case:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ff882f7",
      "metadata": {
        "id": "9ff882f7"
      },
      "outputs": [],
      "source": [
        "#@title Let's prepare the 1D data as before {display-mode: \"form\" }\n",
        "\n",
        "neuron_idx = 91  # The neuron we'll use for binary classification\n",
        "reach_idx1 = 0  # First reach angle\n",
        "reach_idx2 = 3  # Second reach angle\n",
        "\n",
        "# Extract the data for the two angles\n",
        "X_reach1 = Xplan_angles[reach_idx1, :, neuron_idx]\n",
        "X_reach2 = Xplan_angles[reach_idx2, :, neuron_idx]\n",
        "\n",
        "# Let's create a label vector for the two angles\n",
        "y_reach1 = np.zeros(X_reach1.shape[0])  # Label for the first angle\n",
        "y_reach2 = np.ones(X_reach2.shape[0])   # Label for the second angle\n",
        "\n",
        "# Let's also randomly split the data into training and test sets.\n",
        "# We'll keep the same number of trials for each angle.\n",
        "n_train = int(0.8 * num_reaches_per_angle)  # 80% for training\n",
        "# Seed the random number generator for reproducibility\n",
        "np.random.seed(2)\n",
        "train_idxs = np.random.choice(num_reaches_per_angle, n_train, replace=False)\n",
        "# This function just returns the indices that are not already in the training set\n",
        "test_idxs = np.setdiff1d(np.arange(num_reaches_per_angle), train_idxs)\n",
        "\n",
        "# Train data\n",
        "X_train_reach1 = X_reach1[train_idxs]\n",
        "X_train_reach2 = X_reach2[train_idxs]\n",
        "y_train_reach1 = y_reach1[train_idxs]\n",
        "y_train_reach2 = y_reach2[train_idxs]\n",
        "X_train = np.concatenate((X_train_reach1, X_train_reach2))\n",
        "y_train = np.concatenate((y_train_reach1, y_train_reach2))\n",
        "\n",
        "# Test data\n",
        "X_test_reach1 = X_reach1[test_idxs]\n",
        "X_test_reach2 = X_reach2[test_idxs]\n",
        "y_test_reach1 = y_reach1[test_idxs]\n",
        "y_test_reach2 = y_reach2[test_idxs]\n",
        "X_test = np.concatenate((X_test_reach1, X_test_reach2))\n",
        "y_test = np.concatenate((y_test_reach1, y_test_reach2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d13a659",
      "metadata": {
        "id": "1d13a659"
      },
      "outputs": [],
      "source": [
        "# And now let's start playing with the sigmoid function on our data\n",
        "@interact(w=widgets.FloatSlider(min=0, max=5, step=0.05, value=0.5),\n",
        "          b=widgets.FloatSlider(min=-12, max=0, step=0.5, value=-2.0))\n",
        "def plot_sigmoid_on_data(w, b):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    # Run the data through a logistic sigmoid classifier\n",
        "    # Train\n",
        "    z_train = w * X_train + b\n",
        "    sigmoid_train = sigmoid(z_train)\n",
        "    y_pred_train = (sigmoid_train > 0.5).astype(int)\n",
        "    acc_train = accuracy(y_train, y_pred_train)\n",
        "\n",
        "    # Test\n",
        "    z_test = w * X_test + b\n",
        "    sigmoid_test = sigmoid(z_test)\n",
        "    y_pred_test = (sigmoid_test > 0.5).astype(int)\n",
        "    acc_test = accuracy(y_test, y_pred_test)\n",
        "\n",
        "    # Compute the sigmoid function for a range of x values\n",
        "    x_vals = np.linspace(0, 12, 100)\n",
        "    z_vals = w * x_vals + b\n",
        "    sigmoid_vals = sigmoid(z_vals)\n",
        "\n",
        "    # Plot the sigmoid function for the training data\n",
        "    plt.axhline(0.5, color='gray', linestyle='--')\n",
        "    # Plot the vertical line where the sigmoid crosses 0.5\n",
        "    plt.axvline(-b/w if w != 0 else 0, color='gray', linestyle='--')\n",
        "    plt.plot(x_vals, sigmoid_vals, color='b')\n",
        "\n",
        "    # Replot the training data\n",
        "    # Spike counts are integer values, and so many data points will overlap.\n",
        "    # Let's add an offset and a small amount of jitter to the y-axis, purely for\n",
        "    # visualization purposes.\n",
        "    # Seed the random number generator for reproducibility\n",
        "    np.random.seed(0)\n",
        "    plt.plot(\n",
        "        X_train_reach1,\n",
        "        np.random.normal(0, 0.02, size=y_train_reach1.shape),\n",
        "        'r.',\n",
        "        label=f'{angles[reach_idx1]}¬∞ reach',\n",
        "    )\n",
        "    plt.plot(\n",
        "        X_train_reach2,\n",
        "        1.0 + np.random.normal(0, 0.02, size=y_train_reach2.shape),\n",
        "        'b.',\n",
        "        label=f'{angles[reach_idx2]}¬∞ reach',\n",
        "    )\n",
        "\n",
        "    # Final plot formatting\n",
        "    plt.xlabel(f'Spike Count, Neuron {neuron_idx}')\n",
        "    plt.ylabel(f'Probability of Reach {angles[reach_idx2]}¬∞')\n",
        "    plt.title(f\"Logistic Classifier: w={w:.2f}, b={b:.1f}; Train acc: {acc_train:.3f}, Test acc: {acc_test:.3f}\")\n",
        "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64db3c54",
      "metadata": {
        "id": "64db3c54"
      },
      "source": [
        "**_üí¨ Discussion questions:_**\n",
        "\n",
        "1. At which values of $w$ and $b$ does this classifier perform best? Is that performance the same or different from our simple threshold classifier before?\n",
        "2. Is there a range of values for $w$ and $b$ where the classifier still performs well? If so, roughly, what are those ranges?\n",
        "3. For one of the good classifiers you found, what is the probability of class 1 for a data point with a spike counts of the following values:\n",
        "    - 0\n",
        "    - 10\n",
        "    - 2\n",
        "\n",
        "  Think about the proportion of red points and blue points at each spike count value. How do the probabilities you found compare to the proportions of red and blue points at those spike counts?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7bab7f7",
      "metadata": {
        "id": "b7bab7f7"
      },
      "source": [
        "### Two-dimensional logistic classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3cdb55f",
      "metadata": {
        "id": "f3cdb55f"
      },
      "source": [
        "So how do we extend the logistic classifier to two dimensions? We can use the same\n",
        "logistic sigmoid function, but now with two weights and a bias:\n",
        "$$P(C_1 | \\mathbf{x}) = \\sigma(w_1 x_1 + w_2 x_2 + b) = \\frac{1}{1 + e^{-(w_1 x_1 + w_2 x_2 + b)}}$$\n",
        "\n",
        "We can also rewrite this expression with vectors as follows:\n",
        "$$P(C_1 | \\mathbf{x}) = \\sigma(\\mathbf{w}^{\\intercal} \\mathbf{x} + b) = \\frac{1}{1 + e^{-(\\mathbf{w}^{\\intercal} \\mathbf{x} + b)}}$$\n",
        "where $\\mathbf{w} = [w_1, w_2]^{\\intercal}$ is a vector of weights, and $\\mathbf{x} = [x_1, x_2]^{\\intercal}$ is a vector of input features.\n",
        "\n",
        "Just as with the 1D case, we can define the probability of class 2 as:\n",
        "$$P(C_2 | \\mathbf{x}) = 1 - P(C_1 | \\mathbf{x}) = 1 - \\sigma(\\mathbf{w}^{\\intercal} \\mathbf{x} + b)$$\n",
        "\n",
        "And we can use the same thresholding rule to classify the data points:\n",
        "$$\\text{If } P(C_1 | \\mathbf{x}) > 0.5, \\text{ then predict class 1. Else, predict class 2.}$$\n",
        "\n",
        "Let's then explore the behavior of this classifier on our 2D data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90377ab8",
      "metadata": {
        "id": "90377ab8"
      },
      "outputs": [],
      "source": [
        "#@title Let's prepare the 2D data as before {display-mode: \"form\" }\n",
        "\n",
        "neuron_idxs = [91, 34]  # The neurons we'll use for binary classification\n",
        "reach_idx1 = 0  # First reach angle\n",
        "reach_idx2 = 3  # Second reach angle\n",
        "\n",
        "# Extract the data for the two angles\n",
        "X_reach1 = Xplan_angles[reach_idx1, :, neuron_idxs].T\n",
        "X_reach2 = Xplan_angles[reach_idx2, :, neuron_idxs].T\n",
        "\n",
        "# Let's create a label vector for the two angles\n",
        "y_reach1 = np.zeros(X_reach1.shape[0])  # Label for the first angle\n",
        "y_reach2 = np.ones(X_reach2.shape[0])  # Label for the second angle\n",
        "\n",
        "# Let's also randomly split the data into training and test sets.\n",
        "# We'll keep the same number of trials for each angle.\n",
        "n_train = int(0.8 * num_reaches_per_angle)  # 80% for training\n",
        "# Seed the random number generator for reproducibility\n",
        "np.random.seed(2)\n",
        "train_idxs = np.random.choice(num_reaches_per_angle, n_train, replace=False)\n",
        "# This function just returns the indices that are not already in the training set\n",
        "test_idxs = np.setdiff1d(np.arange(num_reaches_per_angle), train_idxs)\n",
        "\n",
        "# Train data\n",
        "X_train_reach1 = X_reach1[train_idxs]\n",
        "X_train_reach2 = X_reach2[train_idxs]\n",
        "y_train_reach1 = y_reach1[train_idxs]\n",
        "y_train_reach2 = y_reach2[train_idxs]\n",
        "X_train = np.concatenate((X_train_reach1, X_train_reach2))\n",
        "y_train = np.concatenate((y_train_reach1, y_train_reach2))\n",
        "\n",
        "# Test data\n",
        "X_test_reach1 = X_reach1[test_idxs]\n",
        "X_test_reach2 = X_reach2[test_idxs]\n",
        "y_test_reach1 = y_reach1[test_idxs]\n",
        "y_test_reach2 = y_reach2[test_idxs]\n",
        "X_test = np.concatenate((X_test_reach1, X_test_reach2))\n",
        "y_test = np.concatenate((y_test_reach1, y_test_reach2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce0a5574",
      "metadata": {
        "id": "ce0a5574"
      },
      "outputs": [],
      "source": [
        "# Let's play around with all three parameters, w_1, w_2, and b, of the decision boundary\n",
        "@interact(w1=widgets.FloatSlider(min=-2, max=2, step=0.1, value=-0.5),\n",
        "          w2=widgets.FloatSlider(min=-2, max=2, step=0.1, value=0.5),\n",
        "          b=widgets.FloatSlider(min=-12, max=0, step=0.5, value=-2.0))\n",
        "def plot_decision_boundary(w1, w2, b):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    # Replot the training data\n",
        "    # Spike counts are integer values, and so many data points will overlap.\n",
        "    # Let's add an offset and a small amount of jitter to the y-axis, purely for\n",
        "    # visualization purposes.\n",
        "    # Seed the random number generator for reproducibility\n",
        "    np.random.seed(0)\n",
        "    plt.plot(\n",
        "        X_train_reach1[:, 0] + np.random.normal(0, 0.05, size=X_train_reach1.shape[0]),\n",
        "        X_train_reach1[:, 1] + np.random.normal(0, 0.05, size=X_train_reach1.shape[0]),\n",
        "        'r.',\n",
        "        label=f'{angles[reach_idx1]}¬∞ reach',\n",
        "    )\n",
        "    plt.plot(\n",
        "        X_train_reach2[:, 0] + np.random.normal(0, 0.05, size=X_train_reach2.shape[0]),\n",
        "        X_train_reach2[:, 1] + np.random.normal(0, 0.05, size=X_train_reach2.shape[0]),\n",
        "        'b.',\n",
        "        label=f'{angles[reach_idx2]}¬∞ reach',\n",
        "    )\n",
        "    # Plot the heat map based on the logistic sigmoid function in 2D\n",
        "    x_vals = np.linspace(X_train[:, 0].min()-1, X_train[:, 0].max()+1, 100)\n",
        "    y_vals = np.linspace(X_train[:, 1].min()-1, X_train[:, 1].max()+1, 100)\n",
        "    xx, yy = np.meshgrid(x_vals, y_vals)\n",
        "    z_vals = w1 * xx + w2 * yy + b\n",
        "    sigmoid_vals = sigmoid(z_vals)\n",
        "\n",
        "    # Plot grid and associated sigmoid values\n",
        "    plt.pcolormesh(xx, yy, sigmoid_vals, shading='auto', cmap='coolwarm_r', alpha=0.8)\n",
        "    plt.colorbar(label='Probability of Class 1')\n",
        "\n",
        "    # Predict the labels for the train data\n",
        "    z_train = X_train @ np.array([w1, w2]) + b\n",
        "    sigmoid_train = sigmoid(z_train)\n",
        "    y_pred_train = (sigmoid_train > 0.5).astype(int)\n",
        "\n",
        "    # Predict the labels for the test data\n",
        "    z_test = X_test @ np.array([w1, w2]) + b\n",
        "    sigmoid_test = sigmoid(z_test)\n",
        "    y_pred_test = (sigmoid_test > 0.5).astype(int)\n",
        "\n",
        "    # Train accuracy\n",
        "    acc_train = accuracy(y_train, y_pred_train)\n",
        "\n",
        "    # Test accuracy\n",
        "    acc_test = accuracy(y_test, y_pred_test)\n",
        "\n",
        "    # Final plot formatting\n",
        "    plt.xlabel(f'Spike Count, Neuron {neuron_idxs[0]}')\n",
        "    plt.ylabel(f'Spike Count, Neuron {neuron_idxs[1]}')\n",
        "    plt.title(f\"w1={w1:.2f}, w2={w2:.2f}, b={b:.1f}; Train acc: {acc_train:.3f}, Test acc: {acc_test:.3f}\")\n",
        "    plt.legend(loc='lower right', frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67bca23a",
      "metadata": {
        "id": "67bca23a"
      },
      "source": [
        "**_üí¨ Discussion questions:_**\n",
        "\n",
        "1. Where is the decision boundary in the above plot? What is it's shape? At the boundary, which class has a higher probability?\n",
        "2. At which values of $w_1$, $w_2$, and $b$ does this classifier perform best? Is that performance the same or different from our simple threshold classifier before?\n",
        "3. Was it harder or easier to find good values for $w_1$, $w_2$, and $b$ than it was to find good values for $w$ and $b$ in the 1D case? Why?\n",
        "4. Suppose we next used three neurons, instead of two neurons to classify these data. How many parameters will be in the three-neuron model? What if we used 100 neurons?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b15621b",
      "metadata": {
        "id": "2b15621b"
      },
      "source": [
        "### Logistic regression\n",
        "\n",
        "You may have noticed that it gets harder to find good values for the weight and\n",
        "bias parameters as the data become more complex: either higher-dimensional, or more\n",
        "difficult to separate by class. Can we instead automate the procedure for finding\n",
        "good parameter values?\n",
        "\n",
        "This brings us to the concept of *logistic regression*, a *supervised learning* method\n",
        "that learns the parameters of a logistic classifier from labeled training data.\n",
        "\n",
        "To train a logistic regression model, we need the following key components:\n",
        "1. **Training Data**: A set of labeled data points, where each point has a feature\n",
        "vector $\\mathbf{x}$ and a corresponding class label $y$.\n",
        "2. **Objective Function** or **Loss Function**: A function that quantifies how well the\n",
        "model's predictions match the true labels.\n",
        "3. **Optimization Algorithm**: An algorithm to minimize the loss function and find the\n",
        "optimal parameters.\n",
        "\n",
        "Then once we fit the model to the training data, we want to evaluate its performance\n",
        "on unseen test data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d032f6a8",
      "metadata": {
        "id": "d032f6a8"
      },
      "source": [
        "### The logistic regression objective function\n",
        "\n",
        "**_üí¨ Discussion questions:_**\n",
        "\n",
        "1. What is our objective with classification?\n",
        "2. Could we use that objective to train a classification model? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e580bda",
      "metadata": {
        "id": "6e580bda"
      },
      "source": [
        "We cannot use the classification accuracy directly as our objective function for\n",
        "training a classification model because it is not a differentiable function. We\n",
        "therefore cannot compute gradients with respect to the model parameters.\n",
        "\n",
        "We then want an objective function that is smooth and differentiable, making it\n",
        "suitable for optimization algorithms like gradient descent.\n",
        "\n",
        "This is where probabilities again provide a significant advantage. We can use the\n",
        "probabilities we defined earlier to create such an objective function.\n",
        "\n",
        "Let's start by defining the likelihood of the data given the model parameters.\n",
        "- Likelihood: The probability of observing the training data given the model parameters.\n",
        "- The likelihood for a single data point is given by:\n",
        "$$P(y_i | \\mathbf{x}_i, \\mathbf{w}, b) = P(C_1 | \\mathbf{x}_i)^{y_i} P(C_2 | \\mathbf{x}_i)^{1 - y_i}$$\n",
        "where $y_i$ is the true class label for data point $\\mathbf{x}_i$ (1 for class 1, 0 for class 2), and $P(C_1 | \\mathbf{x}_i)$ is the predicted probability of class 1 given the input features $\\mathbf{x}_i$.\n",
        "- If we assume that the data points are independent, the likelihood of the entire dataset is the product of the individual likelihoods:\n",
        "$$P(\\mathbf{y} | \\mathbf{X}, \\mathbf{w}, b) = \\prod_{i=1}^{n} P(y_i | \\mathbf{x}_i, \\mathbf{w}, b) = \\prod_{i=1}^{n} P(C_1 | \\mathbf{x}_i)^{y_i} P(C_2 | \\mathbf{x}_i)^{1 - y_i}$$\n",
        "where $\\mathbf{y}$ is the vector of true class labels for all data points, $\\mathbf{X}$ is the matrix of input features, and $n$ is the number of data points.\n",
        "\n",
        "We seek a model that maximizes this likelihood function. This approach is known as *maximum likelihood estimation* (MLE). We could try to optimize this likelihood function directly. We will instead take its logarithm:\n",
        "$$\\log P(\\mathbf{y} | \\mathbf{X}, \\mathbf{w}, b) = \\sum_{i=1}^{n} \\left[ y_i \\log(P(C_1 | \\mathbf{x}_i)) + (1 - y_i) \\log(P(C_2 | \\mathbf{x}_i)) \\right]$$\n",
        "\n",
        "We take the logarithm for two reasons:\n",
        "1. It transforms the product into a sum, which is easier to work with mathematically and numerically.\n",
        "2. The parameters that maximize the log-likelihood will also maximize the original likelihood function, so we can use the log-likelihood as our objective function, and arrive at the same set of parameters.\n",
        "\n",
        "In the machine learning literature, the common practice is to *minimize* a *loss function*. As one final step, then, we can define the loss function as the negative log-likelihood, and also normalize by the number of training data points:\n",
        "$$L(\\mathbf{w}, b) = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ (1 - y_i) \\log(P(C_1 | \\mathbf{x}_i)) + y_i \\log(P(C_2 | \\mathbf{x}_i)) \\right]$$\n",
        "\n",
        "This loss function is commonly known as the *cross-entropy loss* or *logistic loss*.\n",
        "\n",
        "Recall that the predicted probabilities are given by the sigmoid function:\n",
        "$$P(C_1 | \\mathbf{x}_i) = \\sigma(\\mathbf{w}^{\\intercal} \\mathbf{x}_i + b)$$\n",
        "$$P(C_2 | \\mathbf{x}_i) = 1 - \\sigma(\\mathbf{w}^{\\intercal} \\mathbf{x}_i + b)$$\n",
        "\n",
        "If we then plug in the expression for the predicted probabilities, we get, as a complete\n",
        "expression for the loss function:\n",
        "$$L(\\mathbf{w}, b) = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i  \\cdot \\log\\left(\\sigma(\\mathbf{w}^{\\intercal} \\mathbf{x}_i + b)\\right) + (1 - y_i) \\cdot \\log\\left(1 - \\sigma(\\mathbf{w}^{\\intercal} \\mathbf{x}_i + b)\\right) \\right]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52217ddf",
      "metadata": {
        "id": "52217ddf"
      },
      "source": [
        "#### Logistic loss function intuition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01071aa1",
      "metadata": {
        "id": "01071aa1"
      },
      "outputs": [],
      "source": [
        "#@title Let's go back to our 1D data {display-mode: \"form\" }\n",
        "\n",
        "neuron_idx = 91  # The neuron we'll use for binary classification\n",
        "reach_idx1 = 0  # First reach angle\n",
        "reach_idx2 = 3  # Second reach angle\n",
        "\n",
        "# Extract the data for the two angles\n",
        "X_reach1 = Xplan_angles[reach_idx1, :, neuron_idx]\n",
        "X_reach2 = Xplan_angles[reach_idx2, :, neuron_idx]\n",
        "\n",
        "# Let's create a label vector for the two angles\n",
        "y_reach1 = np.zeros(X_reach1.shape[0])  # Label for the first angle\n",
        "y_reach2 = np.ones(X_reach2.shape[0])   # Label for the second angle\n",
        "\n",
        "# Let's also randomly split the data into training and test sets.\n",
        "# We'll keep the same number of trials for each angle.\n",
        "n_train = int(0.8 * num_reaches_per_angle)  # 80% for training\n",
        "# Seed the random number generator for reproducibility\n",
        "np.random.seed(2)\n",
        "train_idxs = np.random.choice(num_reaches_per_angle, n_train, replace=False)\n",
        "# This function just returns the indices that are not already in the training set\n",
        "test_idxs = np.setdiff1d(np.arange(num_reaches_per_angle), train_idxs)\n",
        "\n",
        "# Train data\n",
        "X_train_reach1 = X_reach1[train_idxs]\n",
        "X_train_reach2 = X_reach2[train_idxs]\n",
        "y_train_reach1 = y_reach1[train_idxs]\n",
        "y_train_reach2 = y_reach2[train_idxs]\n",
        "X_train = np.concatenate((X_train_reach1, X_train_reach2))\n",
        "y_train = np.concatenate((y_train_reach1, y_train_reach2))\n",
        "\n",
        "# Test data\n",
        "X_test_reach1 = X_reach1[test_idxs]\n",
        "X_test_reach2 = X_reach2[test_idxs]\n",
        "y_test_reach1 = y_reach1[test_idxs]\n",
        "y_test_reach2 = y_reach2[test_idxs]\n",
        "X_test = np.concatenate((X_test_reach1, X_test_reach2))\n",
        "y_test = np.concatenate((y_test_reach1, y_test_reach2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20dcae91",
      "metadata": {
        "id": "20dcae91"
      },
      "outputs": [],
      "source": [
        "# Let's implement the cross-entropy loss function\n",
        "def cross_entropy_loss(y_true: np.ndarray, w: np.ndarray, b: float, X: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Compute the cross-entropy loss for logistic regression.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : numpy array, shape (n,)\n",
        "        True labels (0 or 1).\n",
        "    w : numpy array, shape (d,)\n",
        "        Weights of the logistic regression model.\n",
        "    b : float\n",
        "        Bias term of the logistic regression model.\n",
        "    X : numpy array, shape (n, d)\n",
        "        Input features.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    loss : float\n",
        "        The cross-entropy loss.\n",
        "    \"\"\"\n",
        "    # If w is a float, convert it to a 1D array\n",
        "    if np.isscalar(w):\n",
        "        w = np.array([w])\n",
        "    if len(X.shape) == 1:\n",
        "        X = X.reshape(-1, 1)\n",
        "    z = X @ w + b\n",
        "    p = sigmoid(z)\n",
        "    # Clip for numerical stability\n",
        "    p = np.clip(p, 1e-12, 1 - 1e-12)\n",
        "    # Compute the cross-entropy loss\n",
        "    loss = -np.mean(y_true * np.log(p) + (1 - y_true) * np.log(1 - p))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45457bb2",
      "metadata": {
        "id": "45457bb2"
      },
      "outputs": [],
      "source": [
        "# And now let's start playing with the sigmoid function on our data\n",
        "@interact(w=widgets.FloatSlider(min=-5, max=5, step=0.05, value=0.5),\n",
        "          b=widgets.FloatSlider(min=-12, max=0, step=0.5, value=-2.0))\n",
        "def plot_sigmoid_on_data(w, b):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    # Run the data through a logistic sigmoid classifier\n",
        "    # Train\n",
        "    z_train = w * X_train + b\n",
        "    sigmoid_train = sigmoid(z_train)\n",
        "    y_pred_train = (sigmoid_train > 0.5).astype(int)\n",
        "    acc_train = accuracy(y_train, y_pred_train)\n",
        "    log_likelihood_train = -cross_entropy_loss(y_train, w, b, X_train)\n",
        "\n",
        "    # Test\n",
        "    z_test = w * X_test + b\n",
        "    sigmoid_test = sigmoid(z_test)\n",
        "    y_pred_test = (sigmoid_test > 0.5).astype(int)\n",
        "    acc_test = accuracy(y_test, y_pred_test)\n",
        "    log_likelihood_test = -cross_entropy_loss(y_test, w, b, X_test)\n",
        "\n",
        "    # Compute the sigmoid function for a range of x values\n",
        "    x_vals = np.linspace(0, 12, 100)\n",
        "    z_vals = w * x_vals + b\n",
        "    sigmoid_vals = sigmoid(z_vals)\n",
        "\n",
        "    # Plot the sigmoid function for the training data\n",
        "    plt.axhline(0.5, color='gray', linestyle='--')\n",
        "    # Plot the vertical line where the sigmoid crosses 0.5\n",
        "    plt.axvline(-b/w if w != 0 else 0, color='gray', linestyle='--')\n",
        "    plt.plot(x_vals, sigmoid_vals, color='b')\n",
        "\n",
        "    # Replot the training data\n",
        "    # Spike counts are integer values, and so many data points will overlap.\n",
        "    # Let's add an offset and a small amount of jitter to the y-axis, purely for\n",
        "    # visualization purposes.\n",
        "    # Seed the random number generator for reproducibility\n",
        "    np.random.seed(0)\n",
        "    plt.plot(\n",
        "        X_train_reach1,\n",
        "        np.random.normal(0, 0.02, size=y_train_reach1.shape),\n",
        "        'r.',\n",
        "        label=f'{angles[reach_idx1]}¬∞ reach',\n",
        "    )\n",
        "    plt.plot(\n",
        "        X_train_reach2,\n",
        "        1.0 + np.random.normal(0, 0.02, size=y_train_reach2.shape),\n",
        "        'b.',\n",
        "        label=f'{angles[reach_idx2]}¬∞ reach',\n",
        "    )\n",
        "\n",
        "    # Final plot formatting\n",
        "    plt.xlabel(f'Spike Count, Neuron {neuron_idx}')\n",
        "    plt.ylabel(f'Probability of Reach {angles[reach_idx2]}¬∞')\n",
        "    plt.title(f\"w={w:.2f}, b={b:.1f}; Train acc: {acc_train:.3f}, Train LL: {log_likelihood_train:.3f}, Test acc: {acc_test:.3f}, Test LL: {log_likelihood_test:.3f}\")\n",
        "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86fa9feb",
      "metadata": {
        "id": "86fa9feb"
      },
      "source": [
        "**_üí¨ Discussion questions:_**\n",
        "\n",
        "1. How does the log-likelihood relate to the classification accuracy? As classification accuracy increases, what happens to the log-likelihood?\n",
        "2. At what values of $w$ and $b$ does the log-likelihood reach its maximum? Are these the same values that you identified earlier?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "go52tqKR097U",
      "metadata": {
        "id": "go52tqKR097U"
      },
      "source": [
        "Let's go through the same excericise, but this time, let's fix the bias term, and only adjust the weight. Then we'll also visualize the cross-entropy loss as a function of the weight parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3fc051c",
      "metadata": {
        "id": "d3fc051c"
      },
      "outputs": [],
      "source": [
        "# Precompute the cross-entropy loss curve for all w values\n",
        "b = -3.5  # Fixed bias term\n",
        "w_vals = np.linspace(0, 5, 100)\n",
        "losses = [cross_entropy_loss(y_train, w, b, X_train) for w in w_vals]\n",
        "\n",
        "@interact(w=widgets.FloatSlider(min=0, max=5, step=0.05, value=0.5))\n",
        "def plot_sigmoid_on_data_fixed_b(w):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    # Left subplot: sigmoid classifier\n",
        "    ax1 = axes[0]\n",
        "    z_train = w * X_train + b\n",
        "    sigmoid_train = sigmoid(z_train)\n",
        "    y_pred_train = (sigmoid_train > 0.5).astype(int)\n",
        "    acc_train = accuracy(y_train, y_pred_train)\n",
        "    log_likelihood_train = -cross_entropy_loss(y_train, w, b, X_train)\n",
        "\n",
        "    z_test = w * X_test + b\n",
        "    sigmoid_test = sigmoid(z_test)\n",
        "    y_pred_test = (sigmoid_test > 0.5).astype(int)\n",
        "    acc_test = accuracy(y_test, y_pred_test)\n",
        "    log_likelihood_test = -cross_entropy_loss(y_test, w, b, X_test)\n",
        "\n",
        "    x_vals = np.linspace(0, 12, 100)\n",
        "    z_vals = w * x_vals + b\n",
        "    sigmoid_vals = sigmoid(z_vals)\n",
        "\n",
        "    ax1.axhline(0.5, color='gray', linestyle='--')\n",
        "    ax1.axvline(-b/w if w != 0 else 0, color='gray', linestyle='--')\n",
        "    ax1.plot(x_vals, sigmoid_vals, color='b')\n",
        "    np.random.seed(0)\n",
        "    ax1.plot(\n",
        "        X_train_reach1,\n",
        "        np.random.normal(0, 0.02, size=y_train_reach1.shape),\n",
        "        'r.',\n",
        "        label=f'{angles[reach_idx1]}¬∞ reach',\n",
        "    )\n",
        "    ax1.plot(\n",
        "        X_train_reach2,\n",
        "        1.0 + np.random.normal(0, 0.02, size=y_train_reach2.shape),\n",
        "        'b.',\n",
        "        label=f'{angles[reach_idx2]}¬∞ reach',\n",
        "    )\n",
        "    ax1.set_xlabel(f'Spike Count, Neuron {neuron_idx}')\n",
        "    ax1.set_ylabel(f'Probability of Reach {angles[reach_idx2]}¬∞')\n",
        "    ax1.set_title(f\"w={w:.2f}, b={b:.1f}; Train acc: {acc_train:.3f}, Train LL: {log_likelihood_train:.3f}, Test acc: {acc_test:.3f}, Test LL: {log_likelihood_test:.3f}\")\n",
        "    ax1.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\n",
        "\n",
        "    # Right subplot: cross-entropy loss curve\n",
        "    ax2 = axes[1]\n",
        "    ax2.plot(w_vals, losses, color='purple')\n",
        "    # Plot current cross-entropy as a red dot\n",
        "    current_loss = cross_entropy_loss(y_train, w, b, X_train)\n",
        "    ax2.plot(w, current_loss, 'ro', label='Current loss')\n",
        "    ax2.set_xlabel('Weight (w)')\n",
        "    ax2.set_ylabel('Cross-Entropy Loss')\n",
        "    ax2.set_title(f'Training loss vs. Weight (b={b:.1f})')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c4adebd",
      "metadata": {
        "id": "8c4adebd"
      },
      "source": [
        "#### Automatically finding the optimal parameters with gradient descent\n",
        "\n",
        "We're now ready to define an algorithm to find the optimal parameters $w$ and $b$\n",
        "that minimize the loss function.\n",
        "\n",
        "We will use the gradient descent algorithm, which iteratively updates the parameters\n",
        "using the gradient of the loss function with respect to those parameters.\n",
        "\n",
        "Intuitively, the (negative) gradient points \"downhill\", indicating the direction in\n",
        "which the loss function decreases most rapidly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc28c4d0",
      "metadata": {
        "id": "bc28c4d0"
      },
      "outputs": [],
      "source": [
        "b = -3.5  # Fixed bias term\n",
        "w_vals = np.linspace(0, 5, 100)\n",
        "losses = [cross_entropy_loss(y_train, w, b, X_train) for w in w_vals]\n",
        "w_vals_grad = np.linspace(0, 5, 15)\n",
        "gradients = [X_train.T @ (sigmoid(X_train * w + b) - y_train) / len(y_train) for w in w_vals_grad]\n",
        "\n",
        "# Now plot the loss function and overlay its gradients with respect to w at each point\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(w_vals, losses, color='purple')\n",
        "# Plot the gradients as horizontal arrows along the x-axis\n",
        "for i in range(len(w_vals_grad)):\n",
        "    # Start at the current weight value, draw a horizontal arrow in the direction of the gradient\n",
        "    plt.arrow(w_vals_grad[i], 0, -gradients[i] * 0.1, 0, head_width=0.03, head_length=0.05, fc='r', ec='r')\n",
        "plt.xlabel('Weight (w)')\n",
        "plt.ylabel('Cross-Entropy Loss')\n",
        "plt.title(f'Training loss vs. Weight (b={b:.1f}) with Gradients')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6dfecb3",
      "metadata": {
        "id": "e6dfecb3"
      },
      "source": [
        "The derivations for the gradient of the loss function with respect to the parameters $w$\n",
        "and $b$ require fairly involved algebra and applications of the chain rule in calculus.\n",
        "\n",
        "We therefore take the following expressions for granted. In the 1D case:\n",
        "- The derivative or gradient of the loss function with respect to $w$\n",
        "  is given by:\n",
        "  $$\\frac{\\partial L}{\\partial w} = \\frac{\\partial}{\\partial w} \\left( -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log\\left(\\sigma(w x_i + b)\\right) + (1 - y_i) \\log\\left(1 - \\sigma(w x_i + b)\\right) \\right] \\right)$$\n",
        "  $$= -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i - \\sigma(w x_i + b) \\right] \\cdot x_i$$\n",
        "- The derivative or gradient of the loss function with respect to $b$\n",
        "  is given by:\n",
        "  $$\\frac{\\partial L}{\\partial b} = \\frac{\\partial}{\\partial b} \\left( -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log\\left(\\sigma(w x_i + b)\\right) + (1 - y_i) \\log\\left(1 - \\sigma(w x_i + b)\\right) \\right] \\right)$$\n",
        "  $$= -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i - \\sigma(w x_i + b) \\right]$$\n",
        "\n",
        "Pseudo-code for the gradient descent algorithm is then as follows:\n",
        "```\n",
        ">>> given learning rate Œ±, and number of iterations num_iter\n",
        ">>> initialize w, b\n",
        ">>> for num_iters:\n",
        ">>>    compute: dL/dw\n",
        ">>>    compute: dL/db\n",
        ">>>    update: w = w - Œ± ‚ãÖ dL/dw\n",
        ">>>    update: b = b - Œ± ‚ãÖ dL/db\n",
        ">>> return w, b\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96586423",
      "metadata": {
        "id": "96586423"
      },
      "source": [
        "#### Putting it all together: the logistic regression classifier\n",
        "\n",
        "We now have all the tools we need to implement a logistic regression classifier.\n",
        "We will create a class that implements the logistic regression algorithm, with a\n",
        "similar structure to the KNN class we created earlier.\n",
        "\n",
        "**_üìù Exercise_**\n",
        "\n",
        "In the code below, implement the gradient descent steps for the weight $w$ and the bias $b$.\n",
        "\n",
        "Wherever you see a `TODO` (there are **two**), attempt to fill in that line of code.\n",
        "\n",
        "You can find the solution below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S0XmWlPT4C57",
      "metadata": {
        "id": "S0XmWlPT4C57"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, lr: float = 0.001, num_iter: int = 1000):\n",
        "        \"\"\"\n",
        "        Initialize the logistic regression model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        lr : float\n",
        "            Learning rate for gradient descent. Defaults to 0.001.\n",
        "        num_iter : int\n",
        "            Number of iterations for gradient descent. Defaults to 1000.\n",
        "        \"\"\"\n",
        "        self.lr = lr\n",
        "        self.num_iter = num_iter\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.loss = cross_entropy_loss\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
        "        \"\"\"\n",
        "        Fit the logistic regression model to the training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : numpy array, shape (n, d)\n",
        "            Input data, where n is the number of samples and d is the number of\n",
        "            features.\n",
        "        y : numpy array, shape (n,)\n",
        "            Target labels (0 or 1) for each sample.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        loss_history : list\n",
        "            List of cross-entropy loss values at each iteration.\n",
        "        weight_history : list of numpy arrays\n",
        "            List of weight vectors at each iteration.\n",
        "        bias_history : list\n",
        "            List of bias values at each iteration.\n",
        "        \"\"\"\n",
        "        # Check if X is a 1D array and reshape it to 2D if necessary\n",
        "        if len(X.shape) == 1:\n",
        "            X = X.reshape(-1, 1)\n",
        "        n, d = X.shape\n",
        "\n",
        "        # Initialize weights and bias to zeros\n",
        "        self.weights = np.zeros(d)\n",
        "        self.bias = 0\n",
        "        # Initialize lists to store the history of weights, bias, and loss\n",
        "        loss_history = []\n",
        "        weight_history = []\n",
        "        bias_history = []\n",
        "\n",
        "        # Main training loop that performs gradient descent\n",
        "        for _ in range(self.num_iter):\n",
        "            # Compute the class probabilities for each data point\n",
        "            logits = X @ self.weights + self.bias\n",
        "            class_probs = sigmoid(logits)\n",
        "            # Compute the cross-entropy loss\n",
        "            loss = self.loss(y, self.weights, self.bias, X)\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            # Compute the gradient of the loss function with respect to weights and bias\n",
        "            dw = (1 / n) * X.T @ (class_probs - y)\n",
        "            db = (1 / n) * np.sum(class_probs - y)\n",
        "\n",
        "            # Perform a gradient descent step\n",
        "            self.weights = None  # TODO: Replace None. Use self.lr and dw\n",
        "            self.bias = None     # TODO: Replace None. Use self.lr and db\n",
        "\n",
        "            # Update the history of weights and bias\n",
        "            weight_history.append(self.weights.copy())\n",
        "            bias_history.append(self.bias)\n",
        "\n",
        "        return loss_history, weight_history, bias_history\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the class labels for the input data using the fitted model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : numpy array, shape (n, d)\n",
        "            Input data, where n is the number of samples and d is the number of\n",
        "            features.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        class_pred : list\n",
        "            List of predicted class labels (0 or 1) for each sample.\n",
        "        \"\"\"\n",
        "        # Check if X is a 1D array and reshape it to 2D if necessary\n",
        "        if len(X.shape) == 1:\n",
        "            X = X.reshape(-1, 1)\n",
        "        # Compute the class probabilities\n",
        "        logits = X @ self.weights + self.bias\n",
        "        class_probs = sigmoid(logits)\n",
        "        # Convert probabilities to class predictions\n",
        "        class_pred = [0 if class_prob <= 0.5 else 1 for class_prob in class_probs]\n",
        "        return class_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00d323a3",
      "metadata": {
        "id": "00d323a3"
      },
      "outputs": [],
      "source": [
        "#@title Double click to see solution {display-mode: \"form\" }\n",
        "class LogisticRegression:\n",
        "    def __init__(self, lr: float = 0.001, num_iter: int = 1000):\n",
        "        \"\"\"\n",
        "        Initialize the logistic regression model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        lr : float\n",
        "            Learning rate for gradient descent. Defaults to 0.001.\n",
        "        num_iter : int\n",
        "            Number of iterations for gradient descent. Defaults to 1000.\n",
        "        \"\"\"\n",
        "        self.lr = lr\n",
        "        self.num_iter = num_iter\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.loss = cross_entropy_loss\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
        "        \"\"\"\n",
        "        Fit the logistic regression model to the training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : numpy array, shape (n, d)\n",
        "            Input data, where n is the number of samples and d is the number of\n",
        "            features.\n",
        "        y : numpy array, shape (n,)\n",
        "            Target labels (0 or 1) for each sample.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        loss_history : list\n",
        "            List of cross-entropy loss values at each iteration.\n",
        "        weight_history : list of numpy arrays\n",
        "            List of weight vectors at each iteration.\n",
        "        bias_history : list\n",
        "            List of bias values at each iteration.\n",
        "        \"\"\"\n",
        "        # Check if X is a 1D array and reshape it to 2D if necessary\n",
        "        if len(X.shape) == 1:\n",
        "            X = X.reshape(-1, 1)\n",
        "        n, d = X.shape\n",
        "\n",
        "        # Initialize weights and bias to zeros\n",
        "        self.weights = np.zeros(d)\n",
        "        self.bias = 0\n",
        "        # Initialize lists to store the history of weights, bias, and loss\n",
        "        loss_history = []\n",
        "        weight_history = []\n",
        "        bias_history = []\n",
        "\n",
        "        # Main training loop that performs gradient descent\n",
        "        for _ in range(self.num_iter):\n",
        "            # Compute the class probabilities for each data point\n",
        "            logits = X @ self.weights + self.bias\n",
        "            class_probs = sigmoid(logits)\n",
        "            # Compute the cross-entropy loss\n",
        "            loss = self.loss(y, self.weights, self.bias, X)\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            # Compute the gradient of the loss function with respect to weights and bias\n",
        "            dw = (1 / n) * X.T @ (class_probs - y)\n",
        "            db = (1 / n) * np.sum(class_probs - y)\n",
        "\n",
        "            # Perform a gradient descent step\n",
        "            self.weights -= self.lr * dw  # SOLUTION\n",
        "            self.bias -= self.lr * db     # SOLUTION\n",
        "\n",
        "            # Update the history of weights and bias\n",
        "            weight_history.append(self.weights.copy())\n",
        "            bias_history.append(self.bias)\n",
        "\n",
        "        return loss_history, weight_history, bias_history\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the class labels for the input data using the fitted model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : numpy array, shape (n, d)\n",
        "            Input data, where n is the number of samples and d is the number of\n",
        "            features.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        class_pred : list\n",
        "            List of predicted class labels (0 or 1) for each sample.\n",
        "        \"\"\"\n",
        "        # Check if X is a 1D array and reshape it to 2D if necessary\n",
        "        if len(X.shape) == 1:\n",
        "            X = X.reshape(-1, 1)\n",
        "        # Compute the class probabilities\n",
        "        logits = X @ self.weights + self.bias\n",
        "        class_probs = sigmoid(logits)\n",
        "        # Convert probabilities to class predictions\n",
        "        class_pred = [0 if class_prob <= 0.5 else 1 for class_prob in class_probs]\n",
        "        return class_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ngLfNKzC1zCi",
      "metadata": {
        "id": "ngLfNKzC1zCi"
      },
      "source": [
        "Now let's create a LogisticRegression instance and fit it to our training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8b4e12e",
      "metadata": {
        "id": "e8b4e12e"
      },
      "outputs": [],
      "source": [
        "log_reg = LogisticRegression(lr=0.1, num_iter=1000)\n",
        "loss_history, weight_history, bias_history = log_reg.fit(X_train, y_train)\n",
        "# Let's predict the labels for the training and test data\n",
        "y_pred_train = log_reg.predict(X_train)\n",
        "y_pred_test = log_reg.predict(X_test)\n",
        "# Compute the accuracy for the training and test data\n",
        "acc_train = accuracy(y_train, y_pred_train)\n",
        "acc_test = accuracy(y_test, y_pred_test)\n",
        "# Print the learned weights and bias\n",
        "print(f'Learned Weights: {log_reg.weights}')\n",
        "print(f'Learned Bias: {log_reg.bias}')\n",
        "# Print the accuracies\n",
        "print(f'Train Accuracy: {acc_train:.3f}')\n",
        "print(f'Test Accuracy: {acc_test:.3f}')\n",
        "\n",
        "# Plot the loss history, and weight and bias evolutions\n",
        "plt.figure(figsize=(12, 3))\n",
        "# make sure subplots don't overlap\n",
        "plt.subplots_adjust(wspace=0.3)\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(loss_history, color='purple')\n",
        "plt.xlabel('Training Iteration')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.title('Loss History during Training')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(weight_history, color='blue')\n",
        "plt.xlabel('Training Iteration')\n",
        "plt.ylabel('Weight')\n",
        "plt.title('Weights Evolution during Training')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(bias_history, color='green')\n",
        "plt.xlabel('Training Iteration')\n",
        "plt.ylabel('Bias')\n",
        "plt.title('Bias Evolution during Training')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fd765e9",
      "metadata": {
        "id": "0fd765e9"
      },
      "source": [
        "**_üí¨ Discussion questions:_**\n",
        "\n",
        "1. How do the parameters learned by this logistic regression classifier compare to the parameters you found earlier by hand?\n",
        "2. How does the performance of this classifier compare to the KNN classifier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bc8a670",
      "metadata": {
        "id": "3bc8a670"
      },
      "outputs": [],
      "source": [
        "#@title Bonus: Animate the learning process! {display-mode: \"form\" }\n",
        "from matplotlib.animation import FuncAnimation\n",
        "def animate_logistic_sigmoid(i):\n",
        "    plt.clf()  # Clear the current figure\n",
        "    w = weight_history[i]\n",
        "    b = bias_history[i]\n",
        "\n",
        "    # Compute the sigmoid function for a range of x values\n",
        "    x_vals = np.linspace(0, 12, 100)\n",
        "    z_vals = w * x_vals + b\n",
        "    sigmoid_vals = sigmoid(z_vals)\n",
        "\n",
        "    # Plot the sigmoid function for the training data\n",
        "    plt.axhline(0.5, color='gray', linestyle='--')\n",
        "    # Plot the vertical line where the sigmoid crosses 0.5\n",
        "    plt.axvline(-b/w if w != 0 else 0, color='gray', linestyle='--')\n",
        "    plt.plot(x_vals, sigmoid_vals, color='b')\n",
        "\n",
        "    # Replot the training data\n",
        "    np.random.seed(0)\n",
        "    plt.plot(\n",
        "        X_train_reach1,\n",
        "        np.random.normal(0, 0.02, size=y_train_reach1.shape),\n",
        "        'r.',\n",
        "        label=f'{angles[reach_idx1]}¬∞ reach',\n",
        "    )\n",
        "    plt.plot(\n",
        "        X_train_reach2,\n",
        "        1.0 + np.random.normal(0, 0.02, size=y_train_reach2.shape),\n",
        "        'b.',\n",
        "        label=f'{angles[reach_idx2]}¬∞ reach',\n",
        "    )\n",
        "\n",
        "    # Final plot formatting\n",
        "    plt.xlabel(f'Spike Count, Neuron {neuron_idx}')\n",
        "    plt.ylabel(f'Probability of Reach {angles[reach_idx2]}¬∞')\n",
        "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\n",
        "    plt.tight_layout()\n",
        "\n",
        "fig = plt.figure(figsize=(8, 5))\n",
        "ani = FuncAnimation(\n",
        "    fig, animate_logistic_sigmoid, frames=len(weight_history), interval=10, repeat=False\n",
        ")\n",
        "# Save the animation as a video file\n",
        "# ani.save('logistic_regression_animation1D.gif', writer='ffmpeg', fps=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d394ea4",
      "metadata": {
        "id": "7d394ea4"
      },
      "source": [
        "#### We can just as well apply our logistic regression model to the 2D data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "483b78d3",
      "metadata": {
        "id": "483b78d3"
      },
      "outputs": [],
      "source": [
        "#@title Back to our 2D data {display-mode: \"form\" }\n",
        "neuron_idxs = [91, 34]  # The neurons we'll use for binary classification\n",
        "reach_idx1 = 0  # First reach angle\n",
        "reach_idx2 = 3  # Second reach angle\n",
        "\n",
        "# Extract the data for the two angles\n",
        "X_reach1 = Xplan_angles[reach_idx1, :, neuron_idxs].T\n",
        "X_reach2 = Xplan_angles[reach_idx2, :, neuron_idxs].T\n",
        "\n",
        "# Let's create a label vector for the two angles\n",
        "y_reach1 = np.zeros(X_reach1.shape[0])  # Label for the first angle\n",
        "y_reach2 = np.ones(X_reach2.shape[0])   # Label for the second angle\n",
        "\n",
        "# Let's also randomly split the data into training and test sets.\n",
        "# We'll keep the same number of trials for each angle.\n",
        "n_train = int(0.8 * num_reaches_per_angle)  # 80% for training\n",
        "# Seed the random number generator for reproducibility\n",
        "np.random.seed(2)\n",
        "train_idxs = np.random.choice(num_reaches_per_angle, n_train, replace=False)\n",
        "# This function just returns the indices that are not already in the training set\n",
        "test_idxs = np.setdiff1d(np.arange(num_reaches_per_angle), train_idxs)\n",
        "\n",
        "# Train data\n",
        "X_train_reach1 = X_reach1[train_idxs]\n",
        "X_train_reach2 = X_reach2[train_idxs]\n",
        "y_train_reach1 = y_reach1[train_idxs]\n",
        "y_train_reach2 = y_reach2[train_idxs]\n",
        "X_train = np.concatenate((X_train_reach1, X_train_reach2))\n",
        "y_train = np.concatenate((y_train_reach1, y_train_reach2))\n",
        "\n",
        "# Test data\n",
        "X_test_reach1 = X_reach1[test_idxs]\n",
        "X_test_reach2 = X_reach2[test_idxs]\n",
        "y_test_reach1 = y_reach1[test_idxs]\n",
        "y_test_reach2 = y_reach2[test_idxs]\n",
        "X_test = np.concatenate((X_test_reach1, X_test_reach2))\n",
        "y_test = np.concatenate((y_test_reach1, y_test_reach2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "varDntbL5txA",
      "metadata": {
        "id": "varDntbL5txA"
      },
      "source": [
        "Create a LogisticRegression instance and fit it to our training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f6f65b9",
      "metadata": {
        "id": "4f6f65b9"
      },
      "outputs": [],
      "source": [
        "log_reg = LogisticRegression(lr=0.1, num_iter=2000)\n",
        "loss_history, weight_history, bias_history = log_reg.fit(X_train, y_train)\n",
        "# Let's predict the labels for the training and test data\n",
        "y_pred_train = log_reg.predict(X_train)\n",
        "y_pred_test = log_reg.predict(X_test)\n",
        "# Compute the accuracy for the training and test data\n",
        "acc_train = accuracy(y_train, y_pred_train)\n",
        "acc_test = accuracy(y_test, y_pred_test)\n",
        "# Print the learned weights and bias\n",
        "print(f'Learned Weights: {log_reg.weights}')\n",
        "print(f'Learned Bias: {log_reg.bias}')\n",
        "# Print the accuracies\n",
        "print(f'Train Accuracy: {acc_train:.3f}')\n",
        "print(f'Test Accuracy: {acc_test:.3f}')\n",
        "\n",
        "# Plot the loss history, and weight and bias evolutions\n",
        "plt.figure(figsize=(12, 3))\n",
        "# make sure subplots don't overlap\n",
        "plt.subplots_adjust(wspace=0.3)\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(loss_history, color='purple')\n",
        "plt.xlabel('Training Iteration')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.title('Loss History during Training')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(weight_history)\n",
        "plt.xlabel('Training Iteration')\n",
        "plt.ylabel('Weight')\n",
        "plt.title('Weights Evolution during Training')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(bias_history, color='green')\n",
        "plt.xlabel('Training Iteration')\n",
        "plt.ylabel('Bias')\n",
        "plt.title('Bias Evolution during Training')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a37a84d1",
      "metadata": {
        "id": "a37a84d1"
      },
      "source": [
        "**_üí¨ Discussion questions:_**\n",
        "\n",
        "1. How do the parameters learned by this logistic regression classifier compare to the\n",
        "   parameters you found earlier by hand?\n",
        "2. How does the performance of this classifier compare to the KNN classifier?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7bfc704",
      "metadata": {
        "id": "f7bfc704"
      },
      "source": [
        "**_üìù Exercises_**\n",
        "\n",
        "1. Rerun the 2D logistic regression problem, but try different values for the learning rate and number of iterations. How does the performance of the classifier change?\n",
        "2. Rerun the above analyses, but try different pairs of reach angles to classify. Which pairs result in the best classification performance, and why? Which pairs result in the the worst classification performance, and why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f479293",
      "metadata": {
        "id": "7f479293"
      },
      "outputs": [],
      "source": [
        "#@title Bonus: Animate the learning process! {display-mode: \"form\" }\n",
        "def animate_logistic_sigmoid_2d(i):\n",
        "    plt.clf()  # Clear the current figure\n",
        "    w1, w2 = weight_history[i]\n",
        "    b = bias_history[i]\n",
        "\n",
        "    # Plot the heat map based on the logistic sigmoid function in 2D\n",
        "    x_vals = np.linspace(X_train[:, 0].min()-1, X_train[:, 0].max()+1, 100)\n",
        "    y_vals = np.linspace(X_train[:, 1].min()-1, X_train[:, 1].max()+1, 100)\n",
        "    xx, yy = np.meshgrid(x_vals, y_vals)\n",
        "    z_vals = w1 * xx + w2 * yy + b\n",
        "    sigmoid_vals = sigmoid(z_vals)\n",
        "\n",
        "    # Plot grid and associated sigmoid values\n",
        "    plt.pcolormesh(xx, yy, sigmoid_vals, shading='auto', cmap='coolwarm_r', alpha=0.8)\n",
        "    plt.colorbar(label='Probability of Class 1')\n",
        "    plt.clim(0, 1)\n",
        "\n",
        "    # Replot the training data\n",
        "    np.random.seed(0)\n",
        "    plt.plot(\n",
        "        X_train_reach1[:, 0] + np.random.normal(0, 0.05, size=X_train_reach1.shape[0]),\n",
        "        X_train_reach1[:, 1] + np.random.normal(0, 0.05, size=X_train_reach1.shape[0]),\n",
        "        'r.',\n",
        "        label=f'{angles[reach_idx1]}¬∞ reach',\n",
        "    )\n",
        "    plt.plot(\n",
        "        X_train_reach2[:, 0] + np.random.normal(0, 0.05, size=X_train_reach2.shape[0]),\n",
        "        X_train_reach2[:, 1] + np.random.normal(0, 0.05, size=X_train_reach2.shape[0]),\n",
        "        'b.',\n",
        "        label=f'{angles[reach_idx2]}¬∞ reach',\n",
        "    )\n",
        "\n",
        "    # Final plot formatting\n",
        "    plt.xlabel(f'Spike Count, Neuron {neuron_idxs[0]}')\n",
        "    plt.ylabel(f'Spike Count, Neuron {neuron_idxs[1]}')\n",
        "    plt.legend(loc='lower right', frameon=False)\n",
        "    plt.tight_layout()\n",
        "\n",
        "fig = plt.figure(figsize=(8, 5))\n",
        "ani = FuncAnimation(fig, animate_logistic_sigmoid_2d, frames=len(weight_history), interval=20, repeat=False)\n",
        "# Save the animation as a video file\n",
        "# ani.save('logistic_regression_animation2D.mp4', writer='ffmpeg', fps=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91b17a21",
      "metadata": {
        "id": "91b17a21"
      },
      "source": [
        "## Bonus topics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "181e806a",
      "metadata": {
        "id": "181e806a"
      },
      "source": [
        "### Population decoding <a name=\"population-decoding\"></a>\n",
        "\n",
        "We have seen how to decode the reach angle from the activity of a single neuron, or\n",
        "from the activity of two neurons. But what if we have many more neurons? How do we\n",
        "decode the reach angle from the activity of a population of neurons?\n",
        "\n",
        "It turns out that we have all the tools we need to do this, with either the KNN\n",
        "classifier or the logistic regression classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f986a078",
      "metadata": {
        "id": "f986a078"
      },
      "outputs": [],
      "source": [
        "# Prepare the data. This time, we'll use all of the neurons in our recording!\n",
        "reach_idx1 = 0  # First reach angle\n",
        "reach_idx2 = 3  # Second reach angle\n",
        "\n",
        "# Extract the data for the two angles\n",
        "X_reach1 = Xplan_angles[reach_idx1, :, :].T\n",
        "X_reach2 = Xplan_angles[reach_idx2, :, :].T\n",
        "\n",
        "# Let's create a label vector for the two angles\n",
        "y_reach1 = np.zeros(X_reach1.shape[0])  # Label for the first angle\n",
        "y_reach2 = np.ones(X_reach2.shape[0])   # Label for the second angle\n",
        "\n",
        "# Let's also randomly split the data into training and test sets.\n",
        "# We'll keep the same number of trials for each angle.\n",
        "n_train = int(0.8 * num_reaches_per_angle)  # 80% for training\n",
        "# Seed the random number generator for reproducibility\n",
        "np.random.seed(2)\n",
        "train_idxs = np.random.choice(num_reaches_per_angle, n_train, replace=False)\n",
        "# This function just returns the indices that are not already in the training set\n",
        "test_idxs = np.setdiff1d(np.arange(num_reaches_per_angle), train_idxs)\n",
        "\n",
        "# Train data\n",
        "X_train_reach1 = X_reach1[train_idxs]\n",
        "X_train_reach2 = X_reach2[train_idxs]\n",
        "y_train_reach1 = y_reach1[train_idxs]\n",
        "y_train_reach2 = y_reach2[train_idxs]\n",
        "X_train = np.concatenate((X_train_reach1, X_train_reach2))\n",
        "y_train = np.concatenate((y_train_reach1, y_train_reach2))\n",
        "\n",
        "# Test data\n",
        "X_test_reach1 = X_reach1[test_idxs]\n",
        "X_test_reach2 = X_reach2[test_idxs]\n",
        "y_test_reach1 = y_reach1[test_idxs]\n",
        "y_test_reach2 = y_reach2[test_idxs]\n",
        "X_test = np.concatenate((X_test_reach1, X_test_reach2))\n",
        "y_test = np.concatenate((y_test_reach1, y_test_reach2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9052bfe",
      "metadata": {
        "id": "e9052bfe"
      },
      "source": [
        "Population decoding with logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cea0fab",
      "metadata": {
        "id": "7cea0fab"
      },
      "outputs": [],
      "source": [
        "# Create a LogisticRegression instance and fit it to our training data\n",
        "log_reg = LogisticRegression(lr=0.1, num_iter=1000)\n",
        "loss_history, weight_history, bias_history = log_reg.fit(X_train, y_train)\n",
        "# Let's predict the labels for the training and test data\n",
        "y_pred_train = log_reg.predict(X_train)\n",
        "y_pred_test = log_reg.predict(X_test)\n",
        "# Compute the accuracy for the training and test data\n",
        "acc_train = accuracy(y_train, y_pred_train)\n",
        "acc_test = accuracy(y_test, y_pred_test)\n",
        "# Print the accuracies\n",
        "print(f'Train Accuracy: {acc_train:.3f}')\n",
        "print(f'Test Accuracy: {acc_test:.3f}')\n",
        "\n",
        "# Plot the loss history, and weight and bias evolutions\n",
        "plt.figure(figsize=(12, 3))\n",
        "# make sure subplots don't overlap\n",
        "plt.subplots_adjust(wspace=0.3)\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(loss_history, color='purple')\n",
        "plt.xlabel('Training Iteration')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.title('Loss History during Training')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(weight_history, color='blue')\n",
        "plt.xlabel('Training Iteration')\n",
        "plt.ylabel('Weight')\n",
        "plt.title('Weights Evolution during Training')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(bias_history, color='green')\n",
        "plt.xlabel('Training Iteration')\n",
        "plt.ylabel('Bias')\n",
        "plt.title('Bias Evolution during Training')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c916b6c9",
      "metadata": {
        "id": "c916b6c9"
      },
      "source": [
        "Population decoding with Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c372431",
      "metadata": {
        "id": "8c372431"
      },
      "outputs": [],
      "source": [
        "knn_ND = KNNBase(k=20)\n",
        "knn_ND.distance = euclidean_distance\n",
        "\n",
        "# Fit the KNN model\n",
        "knn_ND.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the train data\n",
        "y_pred_train = knn_ND.predict_batch(X_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred_test = knn_ND.predict_batch(X_test)\n",
        "\n",
        "# Train accuracy\n",
        "acc_train = accuracy(y_train, y_pred_train)\n",
        "print(f'Train accuracy: {acc_train:.3f}')\n",
        "\n",
        "# Test accuracy\n",
        "acc_test = accuracy(y_test, y_pred_test)\n",
        "print(f'Test accuracy: {acc_test:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8649c5d2",
      "metadata": {
        "id": "8649c5d2"
      },
      "source": [
        "**_üìù Exercises_**\n",
        "\n",
        "1. Rerun the above analyses, but try different pairs of reach angles to classify.\n",
        "2. Across the pairs, try different settings for the KNN and Logistic Regression classifiers. How do the results change? Which classifier performs better? Is that surprising? Why might one classifier be consistently performing better than the other?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ad27e8b",
      "metadata": {
        "id": "8ad27e8b"
      },
      "source": [
        "### Combining classification and PCA <a name=\"class-pca\"></a>\n",
        "\n",
        "We can combine classification and PCA as follows:\n",
        "1. First, we apply PCA to the neural data to reduce its dimensionality.\n",
        "2. Then, we use the reduced-dimensionality data as input to a classification algorithm,\n",
        "   such as KNN or logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "763af4cc",
      "metadata": {
        "id": "763af4cc"
      },
      "outputs": [],
      "source": [
        "# Prepare the data. This time, we'll use all of the neurons in our recording!\n",
        "reach_idx1 = 0  # First reach angle\n",
        "reach_idx2 = 3  # Second reach angle\n",
        "\n",
        "# Extract the data for the two angles\n",
        "X_reach1 = Xplan_angles[reach_idx1, :, :].T\n",
        "X_reach2 = Xplan_angles[reach_idx2, :, :].T\n",
        "\n",
        "# Let's create a label vector for the two angles\n",
        "y_reach1 = np.zeros(X_reach1.shape[0])  # Label for the first angle\n",
        "y_reach2 = np.ones(X_reach2.shape[0])   # Label for the second angle\n",
        "\n",
        "# Let's also randomly split the data into training and test sets.\n",
        "# We'll keep the same number of trials for each angle.\n",
        "n_train = int(0.8 * num_reaches_per_angle)  # 80% for training\n",
        "# Seed the random number generator for reproducibility\n",
        "np.random.seed(2)\n",
        "train_idxs = np.random.choice(num_reaches_per_angle, n_train, replace=False)\n",
        "# This function just returns the indices that are not already in the training set\n",
        "test_idxs = np.setdiff1d(np.arange(num_reaches_per_angle), train_idxs)\n",
        "\n",
        "# Train data\n",
        "X_train_reach1 = X_reach1[train_idxs]\n",
        "X_train_reach2 = X_reach2[train_idxs]\n",
        "y_train_reach1 = y_reach1[train_idxs]\n",
        "y_train_reach2 = y_reach2[train_idxs]\n",
        "X_train = np.concatenate((X_train_reach1, X_train_reach2))\n",
        "y_train = np.concatenate((y_train_reach1, y_train_reach2))\n",
        "\n",
        "# Test data\n",
        "X_test_reach1 = X_reach1[test_idxs]\n",
        "X_test_reach2 = X_reach2[test_idxs]\n",
        "y_test_reach1 = y_reach1[test_idxs]\n",
        "y_test_reach2 = y_reach2[test_idxs]\n",
        "X_test = np.concatenate((X_test_reach1, X_test_reach2))\n",
        "y_test = np.concatenate((y_test_reach1, y_test_reach2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b9aaf36",
      "metadata": {
        "id": "8b9aaf36"
      },
      "source": [
        "Apply PCA to these two reach angles and visualize 2D projections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f241768",
      "metadata": {
        "id": "5f241768"
      },
      "outputs": [],
      "source": [
        "# Apply PCA to these two reach angles and visualize the 2D projections\n",
        "\n",
        "# First find the mean over trials\n",
        "mu = np.mean(X_train, axis=0)\n",
        "\n",
        "# Now compute the data covariance\n",
        "X_train_centered = X_train - mu                      # Don't forget to center your data first!\n",
        "cov = (1/n) * X_train_centered.T @ X_train_centered  # Now you can compute the covariance\n",
        "\n",
        "# Perform eigendecomposition of the data covariance\n",
        "D, U = np.linalg.eig(cov)  # D = vector of eigenvalues, U = matrix of eigenvectors\n",
        "\n",
        "# Make sure the eigenvalues are sorted (in descending order)\n",
        "idx = np.argsort(D)[::-1]\n",
        "D    = D[idx]\n",
        "\n",
        "# Arrange the eigenvectors according to the magnitude of the eigenvalues\n",
        "U = U[:, idx]\n",
        "\n",
        "# Project the data onto the first two principal components\n",
        "X_train_pca = X_train_centered @ U[:, :2]\n",
        "\n",
        "# Visualize the PCA projection\n",
        "plt.figure(figsize=(8, 5))\n",
        "# Replot the training data\n",
        "# Spike counts are integer values, and so many data points will overlap.\n",
        "# Let's add an offset and a small amount of jitter to the y-axis, purely for\n",
        "# visualization purposes.\n",
        "# Seed the random number generator for reproducibility\n",
        "np.random.seed(0)\n",
        "plt.plot(\n",
        "    X_train_pca[y_train == 0, 0] + np.random.normal(0, 0.05, size=X_train_pca[y_train == 0].shape[0]),\n",
        "    X_train_pca[y_train == 0, 1] + np.random.normal(0, 0.05, size=X_train_pca[y_train == 0].shape[0]),\n",
        "    'r.',\n",
        "    label=f'{angles[reach_idx1]}¬∞ reach',\n",
        ")\n",
        "plt.plot(\n",
        "    X_train_pca[y_train == 1, 0] + np.random.normal(0, 0.05, size=X_train_pca[y_train == 1].shape[0]),\n",
        "    X_train_pca[y_train == 1, 1] + np.random.normal(0, 0.05, size=X_train_pca[y_train == 1].shape[0]),\n",
        "    'b.',\n",
        "    label=f'{angles[reach_idx2]}¬∞ reach',\n",
        ")\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.title(f'PCA Projection of Reach Angles {angles[reach_idx1]}¬∞ and {angles[reach_idx2]}¬∞')\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be41d35c",
      "metadata": {
        "id": "be41d35c"
      },
      "source": [
        "Classify the PCA projections with Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99b8f296",
      "metadata": {
        "id": "99b8f296"
      },
      "outputs": [],
      "source": [
        "# Classify the PCA projections with Logistic Regression\n",
        "\n",
        "# Now let's create a LogisticRegression instance and fit it to our training data\n",
        "log_reg = LogisticRegression(lr=0.1, num_iter=1000)\n",
        "loss_history, weight_history, bias_history = log_reg.fit(X_train_pca, y_train)\n",
        "# Let's predict the labels for the training data\n",
        "y_pred_train = log_reg.predict(X_train_pca)\n",
        "# Compute the accuracy for the training data\n",
        "acc_train = accuracy(y_train, y_pred_train)\n",
        "# Print the accuracies\n",
        "print(f'Train Accuracy: {acc_train:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5e926f7",
      "metadata": {
        "id": "e5e926f7"
      },
      "source": [
        "### Multiclass classification <a name=\"multiclass\"></a>\n",
        "\n",
        "The code we wrote for the KNN classifier is readily applicable to multiclass\n",
        "classification. We can simply extend the majority vote rule to consider all classes,\n",
        "and then classify a test point based on the class with the most votes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "280e2512",
      "metadata": {
        "id": "280e2512"
      },
      "outputs": [],
      "source": [
        "# Let's create labels for all angles\n",
        "labels = np.arange(len(angles))\n",
        "y = np.concatenate([np.full(num_reaches_per_angle, label) for label in labels])\n",
        "\n",
        "# Let's also randomly split the data into training and test sets.\n",
        "# We'll keep the same number of trials for each angle.\n",
        "n_train = int(0.8 * num_reaches_per_angle)  # 80% for training\n",
        "# Seed the random number generator for reproducibility\n",
        "np.random.seed(2)\n",
        "train_idxs = np.random.choice(num_reaches_per_angle, n_train, replace=False)\n",
        "# This function just returns the indices that are not already in the training set\n",
        "test_idxs = np.setdiff1d(np.arange(num_reaches_per_angle), train_idxs)\n",
        "\n",
        "# Train data\n",
        "X_train = Xplan_angles[:, train_idxs, :].reshape(-1, Xplan_angles.shape[2])\n",
        "y = y.reshape(num_angles, -1)\n",
        "y_train = y[:, train_idxs]\n",
        "y_train = y_train.reshape(-1)\n",
        "\n",
        "# Test data\n",
        "X_test = Xplan_angles[:, test_idxs, :].reshape(-1, Xplan_angles.shape[2])\n",
        "y_test = y[:, test_idxs]\n",
        "y_test = y_test.reshape(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fa36b46",
      "metadata": {
        "id": "2fa36b46"
      },
      "source": [
        "Classify this 8-reach data with a KNN classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76170b98",
      "metadata": {
        "id": "76170b98"
      },
      "outputs": [],
      "source": [
        "# Classify this 8-reach data with a KNN classifier\n",
        "knn_ND = KNNBase(k=30)\n",
        "knn_ND.distance = euclidean_distance\n",
        "knn_ND.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the train data\n",
        "y_pred_train = knn_ND.predict_batch(X_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred_test = knn_ND.predict_batch(X_test)\n",
        "\n",
        "# Train accuracy\n",
        "acc_train = accuracy(y_train, y_pred_train)\n",
        "print(f'Train accuracy: {acc_train:.3f}')\n",
        "\n",
        "# Test accuracy\n",
        "acc_test = accuracy(y_test, y_pred_test)\n",
        "print(f'Test accuracy: {acc_test:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a65b3345",
      "metadata": {
        "id": "a65b3345"
      },
      "source": [
        "## Further reading <a name=\"further-reading\"></a>\n",
        "\n",
        "Scikit-Learn's methods:\n",
        "1. [Nearest Neighbors](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification)\n",
        "2. [Logistic Regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\n",
        "\n",
        "Modern decoding applications:\n",
        "1. [High-performance brain-to-text communication via handwriting](https://www.nature.com/articles/s41586-021-03506-2)\n",
        "2. [A generic noninvasive neuromotor interface for human-computer interaction](https://www.biorxiv.org/content/10.1101/2024.02.23.581779v1)\n",
        "\n",
        "Textbook chapters:\n",
        "1. [Pattern Recognition and Machine Learning, Chapter 4](https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)\n",
        "2. [The Elements of Statistical Learning, Chapter 4](https://hastie.su.domains/ElemStatLearn/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d80626e",
      "metadata": {
        "id": "6d80626e"
      },
      "source": [
        "### **About the author: Evren Gokcen**\n",
        "\n",
        "* Postdoctoral Researcher at Carnegie Mellon University, Pittsburgh, PA, USA\n",
        "* My research draws from machine learning, signal processing, and control to better understand how brain areas communicate and work together to perform computations.\n",
        "* Feel free to contact me: egokcen@cmu.edu\n",
        "* You can also find me on [GitHub](https://github.com/egokcen), [LinkedIn](https://www.linkedin.com/in/evrengokcen/), and [Google Scholar](https://scholar.google.com/citations?hl=en&user=fal6YjcAAAAJ)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "fbkNfeNCedVK"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}